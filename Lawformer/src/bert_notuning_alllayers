Some weights of the model checkpoint at ../../../RESOURCE/chinese_wwm_ext_pytorch were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
start loading
end loading
681
681
133
133
133
133
149
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.1118608713150024
0  loss:  0.655526876449585
0  loss:  0.15157781541347504
0  loss:  0.06724701821804047
0  loss:  0.14894390106201172
0  loss:  0.10583563148975372
0  loss:  0.1546887755393982
0  loss:  0.04424687847495079
0  loss:  0.008929702453315258
0  loss:  0.1797429770231247
0  loss:  0.07431995123624802
0  loss:  0.010877001099288464
0  loss:  0.048631902784109116
0  loss:  0.11127880215644836
0  loss:  0.08179967105388641
0  loss:  0.20186114311218262
0  loss:  0.09049294888973236
0  loss:  0.11511716991662979
0  loss:  0.07532641291618347
0  loss:  0.1855154037475586
0  loss:  0.06294496357440948
0  loss:  0.0455942340195179
0  loss:  0.1891472488641739
0  loss:  0.03956041857600212
0  loss:  0.08601481467485428
0  loss:  0.01418198924511671
0  loss:  0.08834807574748993
0  loss:  0.07481611520051956
0  loss:  0.04441891983151436
0  loss:  0.04517500475049019
0  loss:  0.07966005802154541
0  loss:  0.10890679806470871
0  loss:  0.15687763690948486
0  loss:  0.11339645087718964
0  loss:  0.025966666638851166
0  loss:  0.09440920501947403
0  loss:  0.04756010323762894
0  loss:  0.054621342569589615
0  loss:  0.08168662339448929
0  loss:  0.06696129590272903
0  loss:  0.0305514857172966
0  loss:  0.039584726095199585
0  loss:  0.04439190402626991
0  loss:  0.12941834330558777
0  loss:  0.08340298384428024
0  loss:  0.07360278069972992
0  loss:  0.141046404838562
0  loss:  0.025222832337021828
0  loss:  0.051185790449380875
0  loss:  0.08255108445882797
0  loss:  0.10533744096755981
0  loss:  0.020746521651744843
0  loss:  0.2734103798866272
0  loss:  0.016888676211237907
0  loss:  0.020966116338968277
0  loss:  0.22459036111831665
0  loss:  0.16619651019573212
0  loss:  0.18337619304656982
0  loss:  0.015603076666593552
0  loss:  0.11540070176124573
0  loss:  0.08138367533683777
0  loss:  0.09232349693775177
0  loss:  0.11379534751176834
0  loss:  0.06853232532739639
0  loss:  0.06522607058286667
0  loss:  0.07323619723320007
0  loss:  0.02966771088540554
0  loss:  0.011329494416713715
0  loss:  0.07604987174272537
0  loss:  0.11980599164962769
0  loss:  0.05653475597500801
0  loss:  0.07218019664287567
0  loss:  0.0364350825548172
0  loss:  0.05724511295557022
0  loss:  0.0755462795495987
0  loss:  0.21104949712753296
0  loss:  0.07721545547246933
0  loss:  0.048015233129262924
0  loss:  0.05501243472099304
0  loss:  0.04846591129899025
0  loss:  0.02657879702746868
0  loss:  0.10104205459356308
0  loss:  0.07758761197328568
0  loss:  0.028527136892080307
0  loss:  0.07474934309720993
0  loss:  0.046102333813905716
0  loss:  0.04518909379839897
0  loss:  0.03857908770442009
0  loss:  0.14161232113838196
0  loss:  0.05543636530637741
0  loss:  0.06816025823354721
0  loss:  0.03781765699386597
0  loss:  0.04539898782968521
0  loss:  0.04730396345257759
0  loss:  0.08432495594024658
0  loss:  0.07426633685827255
0  loss:  0.07799126207828522
0  loss:  0.09301169216632843
0  loss:  0.19448158144950867
0  loss:  0.16440139710903168
------ 全部罪名 ------
Acc:  0.8190654205607477
mi_f1:  0.7053231939163499
ma_f1:  0.7076270500219088
mi_precision:  0.7586912065439673
mi_recall:  0.6589698046181173
------ 交通肇事 ------
Acc:  0.924433249370277
mi_f1:  0.8753993610223643
ma_f1:  0.8900530192071828
mi_precision:  0.845679012345679
mi_recall:  0.9072847682119205
------ 抢劫 ------
Acc:  0.728494623655914
mi_f1:  0.6211180124223603
ma_f1:  0.6090518278018279
mi_precision:  0.7092198581560284
mi_recall:  0.5524861878453039
------ 抢夺 ------
Acc:  0.7564766839378239
mi_f1:  0.6191950464396285
ma_f1:  0.6270990034987796
mi_precision:  0.6493506493506493
mi_recall:  0.591715976331361
------ 过失致人死亡 ------
Acc:  0.8590425531914894
mi_f1:  0.7481481481481481
ma_f1:  0.7647348444157984
mi_precision:  0.753731343283582
mi_recall:  0.7426470588235294
------ 贪污 ------
Acc:  0.8190045248868778
mi_f1:  0.6315789473684211
ma_f1:  0.6324977436718786
mi_precision:  0.7241379310344828
mi_recall:  0.56
------ 挪用公款 ------
Acc:  0.7960339943342776
mi_f1:  0.6620689655172414
ma_f1:  0.6395851488936595
mi_precision:  0.7741935483870968
mi_recall:  0.5783132530120482
------ 挪用资金 ------
Acc:  0.8452722063037249
mi_f1:  0.775
ma_f1:  0.7466109866865747
mi_precision:  0.8435374149659864
mi_recall:  0.7167630057803468
0   tensor(1.3154, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.2159, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.2197, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2380, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.2952, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.2924, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.3246, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.3975, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4083, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.4157, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.4436, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.2859, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  0.9383244514465332
0  loss:  0.528220534324646
0  loss:  0.4411170780658722
0  loss:  0.17028336226940155
0  loss:  0.26673245429992676
0  loss:  0.30400222539901733
0  loss:  0.06108391284942627
0  loss:  0.17222057282924652
0  loss:  0.10454998165369034
0  loss:  0.07053631544113159
0  loss:  0.07497356832027435
0  loss:  0.04873555898666382
0  loss:  0.07659382373094559
0  loss:  0.01221494935452938
0  loss:  0.10922259092330933
0  loss:  0.029032191261649132
0  loss:  0.045393623411655426
0  loss:  0.06207834556698799
0  loss:  0.047512397170066833
0  loss:  0.06978947669267654
0  loss:  0.1513083577156067
0  loss:  0.025953611359000206
0  loss:  0.07193376123905182
0  loss:  0.024752775207161903
0  loss:  0.15246768295764923
0  loss:  0.05881263315677643
0  loss:  0.07581023871898651
0  loss:  0.0920918881893158
0  loss:  0.178954616189003
0  loss:  0.07617741823196411
0  loss:  0.11981257796287537
0  loss:  0.12361225485801697
0  loss:  0.0754697173833847
0  loss:  0.05673536658287048
0  loss:  0.07661308348178864
0  loss:  0.057174183428287506
0  loss:  0.02243323065340519
0  loss:  0.019095877185463905
0  loss:  0.07645038515329361
0  loss:  0.05356058478355408
0  loss:  0.12016646564006805
0  loss:  0.1299055963754654
0  loss:  0.019771656021475792
0  loss:  0.020351633429527283
0  loss:  0.18424780666828156
0  loss:  0.07476231455802917
0  loss:  0.18879970908164978
0  loss:  0.07638715207576752
0  loss:  0.050464507192373276
0  loss:  0.07520963996648788
0  loss:  0.020001500844955444
0  loss:  0.009944294579327106
0  loss:  0.2656472623348236
0  loss:  0.04867063835263252
0  loss:  0.0714375302195549
0  loss:  0.009908402338624
0  loss:  0.031601693481206894
0  loss:  0.14920730888843536
0  loss:  0.12692946195602417
0  loss:  0.11314995586872101
0  loss:  0.08994938433170319
0  loss:  0.04290161654353142
0  loss:  0.05206022039055824
0  loss:  0.012461952865123749
0  loss:  0.06912071257829666
0  loss:  0.09298300743103027
0  loss:  0.04738171398639679
0  loss:  0.09695993363857269
0  loss:  0.07144317030906677
0  loss:  0.12027623504400253
0  loss:  0.14486418664455414
0  loss:  0.020323673263192177
0  loss:  0.050871703773736954
0  loss:  0.0928218811750412
0  loss:  0.07789923995733261
0  loss:  0.07055172324180603
0  loss:  0.044829048216342926
0  loss:  0.0721597895026207
0  loss:  0.05851839482784271
0  loss:  0.08069953322410583
0  loss:  0.04608646035194397
0  loss:  0.05307929962873459
0  loss:  0.06856498122215271
0  loss:  0.0794333815574646
0  loss:  0.11423877626657486
0  loss:  0.19967983663082123
0  loss:  0.10466332733631134
0  loss:  0.14576835930347443
0  loss:  0.09972740709781647
0  loss:  0.04013802856206894
0  loss:  0.05815402418375015
0  loss:  0.14676125347614288
0  loss:  0.09322507679462433
0  loss:  0.1266651153564453
0  loss:  0.05775627866387367
0  loss:  0.032932281494140625
0  loss:  0.10926379263401031
0  loss:  0.06001342460513115
0  loss:  0.08982568979263306
0  loss:  0.0729006826877594
------ 全部罪名 ------
Acc:  0.8431841475913905
mi_f1:  0.7221664994984954
ma_f1:  0.7231725755750029
mi_precision:  0.7468879668049793
mi_recall:  0.6990291262135923
------ 交通肇事 ------
Acc:  0.9333333333333333
mi_f1:  0.8768115942028986
ma_f1:  0.8843155095184771
mi_precision:  0.8768115942028986
mi_recall:  0.8768115942028986
------ 抢劫 ------
Acc:  0.7991360691144709
mi_f1:  0.6801152737752162
ma_f1:  0.6894471617014969
mi_precision:  0.7151515151515152
mi_recall:  0.6483516483516484
------ 抢夺 ------
Acc:  0.8463414634146341
mi_f1:  0.7058823529411764
ma_f1:  0.7098564425770308
mi_precision:  0.743801652892562
mi_recall:  0.6716417910447762
------ 过失致人死亡 ------
Acc:  0.8355437665782494
mi_f1:  0.7448275862068966
ma_f1:  0.7408230280748662
mi_precision:  0.7448275862068966
mi_recall:  0.7448275862068966
------ 贪污 ------
Acc:  0.8297413793103449
mi_f1:  0.6463878326996197
ma_f1:  0.6383345651638334
mi_precision:  0.7589285714285714
mi_recall:  0.5629139072847682
------ 挪用公款 ------
Acc:  0.8130563798219584
mi_f1:  0.6984126984126984
ma_f1:  0.6716856349026188
mi_precision:  0.7394957983193278
mi_recall:  0.6616541353383458
------ 挪用资金 ------
Acc:  0.8442982456140351
mi_f1:  0.707395498392283
ma_f1:  0.7032694675246349
mi_precision:  0.6707317073170732
mi_recall:  0.7482993197278912
0   tensor(1.3143, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.1981, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.2032, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2294, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.2804, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.2887, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.3337, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.4160, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4128, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.3987, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.4236, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.2717, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0749707221984863
0  loss:  0.38961121439933777
0  loss:  0.35373562574386597
0  loss:  0.18784618377685547
0  loss:  0.10025592893362045
0  loss:  0.03230530768632889
0  loss:  0.018349498510360718
0  loss:  0.07827867567539215
0  loss:  0.08650480955839157
0  loss:  0.046647123992443085
0  loss:  0.1366354078054428
0  loss:  0.11869503557682037
0  loss:  0.09804725646972656
0  loss:  0.19862906634807587
0  loss:  0.15074364840984344
0  loss:  0.058805253356695175
0  loss:  0.054523881524801254
0  loss:  0.07307945936918259
0  loss:  0.05576763302087784
0  loss:  0.0797506719827652
0  loss:  0.06825513392686844
0  loss:  0.14275099337100983
0  loss:  0.049643125385046005
0  loss:  0.08896473050117493
0  loss:  0.04132944718003273
0  loss:  0.08540468662977219
0  loss:  0.11861958354711533
0  loss:  0.07061047106981277
0  loss:  0.08705185353755951
0  loss:  0.1403493732213974
0  loss:  0.08381840586662292
0  loss:  0.054790932685136795
0  loss:  0.03412867337465286
0  loss:  0.11595725268125534
0  loss:  0.08151736855506897
0  loss:  0.08113008737564087
0  loss:  0.08255390077829361
0  loss:  0.21182556450366974
0  loss:  0.06655988842248917
0  loss:  0.05838657543063164
0  loss:  0.1696968376636505
0  loss:  0.10860913246870041
0  loss:  0.1286253184080124
0  loss:  0.18863214552402496
0  loss:  0.10897038877010345
0  loss:  0.2969788610935211
0  loss:  0.04962874576449394
0  loss:  0.05420679599046707
0  loss:  0.07880911231040955
0  loss:  0.11015449464321136
0  loss:  0.12810681760311127
0  loss:  0.038559503853321075
0  loss:  0.10854420065879822
0  loss:  0.09481137990951538
0  loss:  0.08273953944444656
0  loss:  0.06630275398492813
0  loss:  0.0496344268321991
0  loss:  0.04612470045685768
0  loss:  0.05048028379678726
0  loss:  0.04668467491865158
0  loss:  0.09769720584154129
0  loss:  0.03677632287144661
0  loss:  0.06226174160838127
0  loss:  0.05563310161232948
0  loss:  0.07855986803770065
0  loss:  0.08959587663412094
0  loss:  0.02845323644578457
0  loss:  0.07871125638484955
0  loss:  0.05440517142415047
0  loss:  0.11494483798742294
0  loss:  0.0885893777012825
0  loss:  0.032178230583667755
0  loss:  0.15233872830867767
0  loss:  0.05048143118619919
0  loss:  0.12215426564216614
0  loss:  0.07901739329099655
0  loss:  0.08966043591499329
0  loss:  0.09729541838169098
0  loss:  0.024046096950769424
0  loss:  0.12923572957515717
0  loss:  0.11264730244874954
0  loss:  0.06838752329349518
0  loss:  0.09361617267131805
0  loss:  0.062322456389665604
0  loss:  0.031523462384939194
0  loss:  0.1377989500761032
0  loss:  0.14913888275623322
0  loss:  0.171980082988739
0  loss:  0.12241194397211075
0  loss:  0.09819145500659943
0  loss:  0.14628790318965912
0  loss:  0.09972137957811356
0  loss:  0.0867157056927681
0  loss:  0.08267802000045776
0  loss:  0.07185189425945282
0  loss:  0.06895051151514053
0  loss:  0.08670385926961899
0  loss:  0.06717497855424881
0  loss:  0.07181351631879807
0  loss:  0.2589856684207916
------ 全部罪名 ------
Acc:  0.841756709654932
mi_f1:  0.7398843930635837
ma_f1:  0.7474318291076041
mi_precision:  0.7812817904374364
mi_recall:  0.7026532479414456
------ 交通肇事 ------
Acc:  0.92
mi_f1:  0.851063829787234
ma_f1:  0.8657604679802956
mi_precision:  0.8695652173913043
mi_recall:  0.8333333333333334
------ 抢劫 ------
Acc:  0.7715736040609137
mi_f1:  0.6775244299674268
ma_f1:  0.6738691521669536
mi_precision:  0.7482014388489209
mi_recall:  0.6190476190476191
------ 抢夺 ------
Acc:  0.8475
mi_f1:  0.7464788732394365
ma_f1:  0.7574179418475644
mi_precision:  0.7412587412587412
mi_recall:  0.75177304964539
------ 过失致人死亡 ------
Acc:  0.8333333333333334
mi_f1:  0.7395498392282958
ma_f1:  0.7580019606848875
mi_precision:  0.8098591549295775
mi_recall:  0.6804733727810651
------ 贪污 ------
Acc:  0.8159806295399515
mi_f1:  0.7058823529411765
ma_f1:  0.7337352869797354
mi_precision:  0.7132867132867133
mi_recall:  0.6986301369863014
------ 挪用公款 ------
Acc:  0.8366834170854272
mi_f1:  0.7342657342657342
ma_f1:  0.716243961352657
mi_precision:  0.8015267175572519
mi_recall:  0.6774193548387096
------ 挪用资金 ------
Acc:  0.851581508515815
mi_f1:  0.7111111111111111
ma_f1:  0.7045659358942933
mi_precision:  0.7741935483870968
mi_recall:  0.6575342465753424
0   tensor(1.3018, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.1759, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.1838, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2257, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.2736, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.2509, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.3234, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.4030, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4120, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.4250, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.4585, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.2715, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.1158714294433594
0  loss:  0.3224654793739319
0  loss:  0.26013511419296265
0  loss:  0.19284695386886597
0  loss:  0.09112709015607834
0  loss:  0.08381376415491104
0  loss:  0.048048004508018494
0  loss:  0.14838241040706635
0  loss:  0.0996330976486206
0  loss:  0.0373036228120327
0  loss:  0.006617577746510506
0  loss:  0.08076030015945435
0  loss:  0.0674392431974411
0  loss:  0.0560523197054863
0  loss:  0.04566429927945137
0  loss:  0.11097051203250885
0  loss:  0.04710819572210312
0  loss:  0.1427280455827713
0  loss:  0.05012204870581627
0  loss:  0.0770866721868515
0  loss:  0.06566420197486877
0  loss:  0.071412093937397
0  loss:  0.12269064038991928
0  loss:  0.011857840232551098
0  loss:  0.10137749463319778
0  loss:  0.04553084447979927
0  loss:  0.2215840071439743
0  loss:  0.14028802514076233
0  loss:  0.05319003388285637
0  loss:  0.09797767549753189
0  loss:  0.07232610136270523
0  loss:  0.09661515802145004
0  loss:  0.004590002819895744
0  loss:  0.020986629649996758
0  loss:  0.0709446519613266
0  loss:  0.02044105716049671
0  loss:  0.11401530355215073
0  loss:  0.078031986951828
0  loss:  0.1084892526268959
0  loss:  0.08974197506904602
0  loss:  0.04263371229171753
0  loss:  0.06600470840930939
0  loss:  0.059096645563840866
0  loss:  0.11242534220218658
0  loss:  0.06787775456905365
0  loss:  0.08761870115995407
0  loss:  0.12130586802959442
0  loss:  0.0358811654150486
0  loss:  0.08640674501657486
0  loss:  0.11229516565799713
0  loss:  0.06556565314531326
0  loss:  0.05155587196350098
0  loss:  0.0370512455701828
0  loss:  0.051245104521512985
0  loss:  0.07866650819778442
0  loss:  0.04067186638712883
0  loss:  0.053529463708400726
0  loss:  0.04672135412693024
0  loss:  0.08550631999969482
0  loss:  0.09158726036548615
0  loss:  0.07682536542415619
0  loss:  0.05526736378669739
0  loss:  0.13467326760292053
0  loss:  0.04037974402308464
0  loss:  0.14303703606128693
0  loss:  0.03369395434856415
0  loss:  0.0646786019206047
0  loss:  0.07283342629671097
0  loss:  0.018971377983689308
0  loss:  0.06733665615320206
0  loss:  0.03012504242360592
0  loss:  0.046096235513687134
0  loss:  0.010344096459448338
0  loss:  0.14592353999614716
0  loss:  0.03671747446060181
0  loss:  0.0853237509727478
0  loss:  0.07334426790475845
0  loss:  0.055493008345365524
0  loss:  0.0863371342420578
0  loss:  0.06470462679862976
0  loss:  0.033902641385793686
0  loss:  0.03951919078826904
0  loss:  0.10542246699333191
0  loss:  0.08368887007236481
0  loss:  0.08218064159154892
0  loss:  0.20893558859825134
0  loss:  0.01218058168888092
0  loss:  0.11490462720394135
0  loss:  0.13762037456035614
0  loss:  0.022221267223358154
0  loss:  0.12924496829509735
0  loss:  0.03557758405804634
0  loss:  0.25112301111221313
0  loss:  0.07174205034971237
0  loss:  0.1261608600616455
0  loss:  0.033939559012651443
0  loss:  0.04120514169335365
0  loss:  0.10372820496559143
0  loss:  0.027453599497675896
0  loss:  0.1296616643667221
------ 全部罪名 ------
Acc:  0.8453757225433526
mi_f1:  0.7519230769230769
ma_f1:  0.7600420672941736
mi_precision:  0.791497975708502
mi_recall:  0.7161172161172161
------ 交通肇事 ------
Acc:  0.9226804123711341
mi_f1:  0.8296296296296295
ma_f1:  0.8452650626593083
mi_precision:  0.8549618320610687
mi_recall:  0.8057553956834532
------ 抢劫 ------
Acc:  0.8181818181818182
mi_f1:  0.7012987012987013
ma_f1:  0.7131336405529953
mi_precision:  0.7659574468085106
mi_recall:  0.6467065868263473
------ 抢夺 ------
Acc:  0.8211586901763224
mi_f1:  0.7114093959731543
ma_f1:  0.7195577602580279
mi_precision:  0.726027397260274
mi_recall:  0.6973684210526315
------ 过失致人死亡 ------
Acc:  0.8601895734597157
mi_f1:  0.7628865979381444
ma_f1:  0.7644729907773385
mi_precision:  0.8473282442748091
mi_recall:  0.69375
------ 贪污 ------
Acc:  0.8121546961325967
mi_f1:  0.752411575562701
ma_f1:  0.7611491965207136
mi_precision:  0.8068965517241379
mi_recall:  0.7048192771084337
------ 挪用公款 ------
Acc:  0.8222811671087533
mi_f1:  0.7452229299363057
ma_f1:  0.7513157894736842
mi_precision:  0.7452229299363057
mi_recall:  0.7452229299363057
------ 挪用资金 ------
Acc:  0.8612565445026178
mi_f1:  0.7708333333333334
ma_f1:  0.7778541111171573
mi_precision:  0.8102189781021898
mi_recall:  0.7350993377483444
0   tensor(1.3019, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.1943, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.1940, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2167, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.2973, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.2958, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.3454, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.4149, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4145, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.4188, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.4445, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.2797, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0340311527252197
0  loss:  0.3532164990901947
0  loss:  0.27316558361053467
0  loss:  0.08912994712591171
0  loss:  0.1350952386856079
0  loss:  0.21422995626926422
0  loss:  0.04193197190761566
0  loss:  0.06048211827874184
0  loss:  0.0613064207136631
0  loss:  0.026415452361106873
0  loss:  0.12904417514801025
0  loss:  0.11793215572834015
0  loss:  0.02752550132572651
0  loss:  0.03906760737299919
0  loss:  0.024419277906417847
0  loss:  0.06029193475842476
0  loss:  0.12495163828134537
0  loss:  0.04294658079743385
0  loss:  0.086693674325943
0  loss:  0.054989293217659
0  loss:  0.05738876387476921
0  loss:  0.08861102908849716
0  loss:  0.1102680116891861
0  loss:  0.1380607932806015
0  loss:  0.11541265994310379
0  loss:  0.010230095125734806
0  loss:  0.03419728949666023
0  loss:  0.16112233698368073
0  loss:  0.10476519167423248
0  loss:  0.033385321497917175
0  loss:  0.07770176231861115
0  loss:  0.011213481426239014
0  loss:  0.15080709755420685
0  loss:  0.12485004216432571
0  loss:  0.03507717698812485
0  loss:  0.06458869576454163
0  loss:  0.05579894781112671
0  loss:  0.045273516327142715
0  loss:  0.01575583592057228
0  loss:  0.007742519956082106
0  loss:  0.0068280077539384365
0  loss:  0.046753983944654465
0  loss:  0.0570949949324131
0  loss:  0.12133911997079849
0  loss:  0.10243719816207886
0  loss:  0.029670588672161102
0  loss:  0.1105136051774025
0  loss:  0.03903055563569069
0  loss:  0.06755135953426361
0  loss:  0.09620612859725952
0  loss:  0.11219656467437744
0  loss:  0.021559521555900574
0  loss:  0.040333062410354614
0  loss:  0.06605689227581024
0  loss:  0.024489378556609154
0  loss:  0.08456604927778244
0  loss:  0.08094464242458344
0  loss:  0.06046208739280701
0  loss:  0.06278897821903229
0  loss:  0.06747395545244217
0  loss:  0.0716795027256012
0  loss:  0.043783001601696014
0  loss:  0.11459098756313324
0  loss:  0.1401882767677307
0  loss:  0.0400143563747406
0  loss:  0.20728205144405365
0  loss:  0.012741082347929478
0  loss:  0.1807098239660263
0  loss:  0.1395723819732666
0  loss:  0.08904100954532623
0  loss:  0.03475538268685341
0  loss:  0.01501311082392931
0  loss:  0.2379569411277771
0  loss:  0.10050632059574127
0  loss:  0.06334153562784195
0  loss:  0.06383832544088364
0  loss:  0.04959256947040558
0  loss:  0.128980353474617
0  loss:  0.17825061082839966
0  loss:  0.032203879207372665
0  loss:  0.020718583837151527
0  loss:  0.06733076274394989
0  loss:  0.0323750264942646
0  loss:  0.08497065305709839
0  loss:  0.08649684488773346
0  loss:  0.06079406663775444
0  loss:  0.06027601286768913
0  loss:  0.08887788653373718
0  loss:  0.042724087834358215
0  loss:  0.016590848565101624
0  loss:  0.1312260925769806
0  loss:  0.08520878106355667
0  loss:  0.2544282078742981
0  loss:  0.0491471067070961
0  loss:  0.09448091685771942
0  loss:  0.008036386221647263
0  loss:  0.006539037451148033
0  loss:  0.11308751255273819
0  loss:  0.050482187420129776
0  loss:  0.10620326548814774
------ 全部罪名 ------
Acc:  0.8249919276719406
mi_f1:  0.7069040382110292
ma_f1:  0.7187011218598713
mi_precision:  0.7925998052580331
mi_recall:  0.6379310344827587
------ 交通肇事 ------
Acc:  0.9283819628647215
mi_f1:  0.897196261682243
ma_f1:  0.9093214656591787
mi_precision:  0.9411764705882353
mi_recall:  0.8571428571428571
------ 抢劫 ------
Acc:  0.7627494456762749
mi_f1:  0.6218487394957983
ma_f1:  0.6353269217925599
mi_precision:  0.6646706586826348
mi_recall:  0.5842105263157895
------ 抢夺 ------
Acc:  0.834051724137931
mi_f1:  0.6773162939297125
ma_f1:  0.6818780692298478
mi_precision:  0.7019867549668874
mi_recall:  0.654320987654321
------ 过失致人死亡 ------
Acc:  0.8260869565217391
mi_f1:  0.7035830618892509
ma_f1:  0.7063849022737342
mi_precision:  0.782608695652174
mi_recall:  0.6390532544378699
------ 贪污 ------
Acc:  0.7823529411764706
mi_f1:  0.6011904761904762
ma_f1:  0.6209657948999034
mi_precision:  0.7266187050359713
mi_recall:  0.5126903553299492
------ 挪用公款 ------
Acc:  0.8091451292246521
mi_f1:  0.6648199445983379
ma_f1:  0.6790123456790125
mi_precision:  0.8275862068965517
mi_recall:  0.5555555555555556
------ 挪用资金 ------
Acc:  0.8624338624338624
mi_f1:  0.8051948051948052
ma_f1:  0.768584722621443
mi_precision:  0.9253731343283582
mi_recall:  0.7126436781609196
0   tensor(1.2999, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.1886, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.2075, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2339, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.2666, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.2751, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.3069, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.3916, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.3731, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.3692, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.4210, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.2433, device='cuda:0', grad_fn=<CopyBackwards>)
0.725240240322507
