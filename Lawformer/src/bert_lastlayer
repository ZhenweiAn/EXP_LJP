Some weights of the model checkpoint at ../../../RESOURCE/chinese_wwm_ext_pytorch were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
start loading
end loading
681
681
133
133
133
133
149
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  0.993893027305603
0  loss:  0.3591765761375427
0  loss:  0.1959952563047409
0  loss:  0.07167116552591324
0  loss:  0.13986830413341522
0  loss:  0.1850651055574417
0  loss:  0.12247274070978165
0  loss:  0.14009110629558563
0  loss:  0.11873406916856766
0  loss:  0.058810602873563766
0  loss:  0.06248841434717178
0  loss:  0.16022412478923798
0  loss:  0.12234274297952652
0  loss:  0.1409023404121399
0  loss:  0.2504153847694397
0  loss:  0.11609561741352081
0  loss:  0.10898905247449875
0  loss:  0.14015249907970428
0  loss:  0.12450101971626282
0  loss:  0.10977957397699356
0  loss:  0.0946984812617302
0  loss:  0.1811869591474533
0  loss:  0.09058845043182373
0  loss:  0.10673918575048447
0  loss:  0.06497328728437424
0  loss:  0.07206569612026215
0  loss:  0.14770779013633728
0  loss:  0.07815584540367126
0  loss:  0.07933316379785538
0  loss:  0.11925549060106277
0  loss:  0.1430600881576538
0  loss:  0.1418556123971939
0  loss:  0.08905375748872757
0  loss:  0.09213272482156754
0  loss:  0.1587865650653839
0  loss:  0.21487557888031006
0  loss:  0.10737507790327072
0  loss:  0.1069532185792923
0  loss:  0.11301133781671524
0  loss:  0.14143557846546173
0  loss:  0.1187346875667572
0  loss:  0.11743815243244171
0  loss:  0.09315764158964157
0  loss:  0.12504573166370392
0  loss:  0.0654173418879509
0  loss:  0.09978572279214859
0  loss:  0.1560487151145935
0  loss:  0.10054419934749603
0  loss:  0.19904038310050964
0  loss:  0.13653241097927094
0  loss:  0.10151361674070358
0  loss:  0.13794921338558197
0  loss:  0.08591270446777344
0  loss:  0.13369552791118622
0  loss:  0.06096172332763672
0  loss:  0.1600854992866516
0  loss:  0.19073671102523804
0  loss:  0.2488207072019577
0  loss:  0.10484879463911057
0  loss:  0.18530195951461792
0  loss:  0.25107675790786743
0  loss:  0.1822926104068756
0  loss:  0.22055697441101074
0  loss:  0.13468222320079803
0  loss:  0.16245940327644348
0  loss:  0.12693773210048676
0  loss:  0.08596305549144745
0  loss:  0.20509082078933716
0  loss:  0.12692908942699432
0  loss:  0.08721115440130234
0  loss:  0.1273963302373886
0  loss:  0.09415797144174576
0  loss:  0.10006431490182877
0  loss:  0.07973874360322952
0  loss:  0.23300598561763763
0  loss:  0.12716566026210785
0  loss:  0.15860426425933838
0  loss:  0.17221686244010925
0  loss:  0.2541906237602234
0  loss:  0.10542162507772446
0  loss:  0.06504734605550766
0  loss:  0.03368423134088516
0  loss:  0.12442286312580109
0  loss:  0.123963363468647
0  loss:  0.07896100729703903
0  loss:  0.12319884449243546
0  loss:  0.15767556428909302
0  loss:  0.13668286800384521
0  loss:  0.17382819950580597
0  loss:  0.059282682836055756
0  loss:  0.14210666716098785
0  loss:  0.08321024477481842
0  loss:  0.08766154199838638
0  loss:  0.12794555723667145
0  loss:  0.10965989530086517
0  loss:  0.1180483028292656
0  loss:  0.12561075389385223
0  loss:  0.14143094420433044
0  loss:  0.10626492649316788
0  loss:  0.01331139076501131
------ 全部罪名 ------
Acc:  0.7809345794392524
mi_f1:  0.5938144329896907
ma_f1:  0.5864698737775941
mi_precision:  0.7076167076167076
mi_recall:  0.5115452930728241
------ 交通肇事 ------
Acc:  0.8916876574307305
mi_f1:  0.8108108108108109
ma_f1:  0.8160753421198808
mi_precision:  0.8275862068965517
mi_recall:  0.7947019867549668
------ 抢劫 ------
Acc:  0.7016129032258065
mi_f1:  0.503448275862069
ma_f1:  0.44646681565286217
mi_precision:  0.6697247706422018
mi_recall:  0.40331491712707185
------ 抢夺 ------
Acc:  0.7512953367875648
mi_f1:  0.5054945054945055
ma_f1:  0.4919405340872469
mi_precision:  0.6634615384615384
mi_recall:  0.40828402366863903
------ 过失致人死亡 ------
Acc:  0.8218085106382979
mi_f1:  0.6595744680851064
ma_f1:  0.6712222452503447
mi_precision:  0.636986301369863
mi_recall:  0.6838235294117647
------ 贪污 ------
Acc:  0.7714932126696833
mi_f1:  0.4715447154471545
ma_f1:  0.4551454452070785
mi_precision:  0.6041666666666666
mi_recall:  0.38666666666666666
------ 挪用公款 ------
Acc:  0.7422096317280453
mi_f1:  0.4881889763779528
ma_f1:  0.4700922696890439
mi_precision:  0.7045454545454546
mi_recall:  0.37349397590361444
------ 挪用资金 ------
Acc:  0.7793696275071633
mi_f1:  0.6755852842809364
ma_f1:  0.6413238289916143
mi_precision:  0.8015873015873016
mi_recall:  0.5838150289017341
0   tensor(5.2768, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  0.9921952486038208
0  loss:  0.35652777552604675
0  loss:  0.19443289935588837
0  loss:  0.1451469510793686
0  loss:  0.16442812979221344
0  loss:  0.09926890581846237
0  loss:  0.27723729610443115
0  loss:  0.12927737832069397
0  loss:  0.04991781339049339
0  loss:  0.05836154520511627
0  loss:  0.12577496469020844
0  loss:  0.07984698563814163
0  loss:  0.15087644755840302
0  loss:  0.12434443086385727
0  loss:  0.1618392914533615
0  loss:  0.20661911368370056
0  loss:  0.11739186197519302
0  loss:  0.25864145159721375
0  loss:  0.09514354914426804
0  loss:  0.15760564804077148
0  loss:  0.09104262292385101
0  loss:  0.1393880695104599
0  loss:  0.13528312742710114
0  loss:  0.11480662971735
0  loss:  0.16867585480213165
0  loss:  0.24591611325740814
0  loss:  0.13297517597675323
0  loss:  0.22254133224487305
0  loss:  0.12472740560770035
0  loss:  0.11978056281805038
0  loss:  0.16308751702308655
0  loss:  0.1418641358613968
0  loss:  0.15483301877975464
0  loss:  0.08063601702451706
0  loss:  0.2417110651731491
0  loss:  0.23046362400054932
0  loss:  0.11369830369949341
0  loss:  0.14094258844852448
0  loss:  0.15207161009311676
0  loss:  0.07982122898101807
0  loss:  0.12559767067432404
0  loss:  0.06023678183555603
0  loss:  0.1701219528913498
0  loss:  0.2251817137002945
0  loss:  0.07613521814346313
0  loss:  0.1288623809814453
0  loss:  0.21764545142650604
0  loss:  0.10531464964151382
0  loss:  0.13699139654636383
0  loss:  0.08915535360574722
0  loss:  0.2778759300708771
0  loss:  0.1390988975763321
0  loss:  0.22332216799259186
0  loss:  0.07955142110586166
0  loss:  0.11938411742448807
0  loss:  0.23038055002689362
0  loss:  0.08834023773670197
0  loss:  0.0751272514462471
0  loss:  0.2746846377849579
0  loss:  0.23088227212429047
0  loss:  0.1503569781780243
0  loss:  0.1264217346906662
0  loss:  0.16486956179141998
0  loss:  0.12153887003660202
0  loss:  0.19480687379837036
0  loss:  0.056163571774959564
0  loss:  0.10337665677070618
0  loss:  0.07978490740060806
0  loss:  0.2789977192878723
0  loss:  0.07420323044061661
0  loss:  0.102476567029953
0  loss:  0.09560047835111618
0  loss:  0.1347368359565735
0  loss:  0.16361893713474274
0  loss:  0.13026882708072662
0  loss:  0.09282519668340683
0  loss:  0.17106129229068756
0  loss:  0.1224512979388237
0  loss:  0.06775379180908203
0  loss:  0.10925794392824173
0  loss:  0.10849186033010483
0  loss:  0.18553043901920319
0  loss:  0.13765670359134674
0  loss:  0.11941920965909958
0  loss:  0.24793851375579834
0  loss:  0.2549138069152832
0  loss:  0.12728999555110931
0  loss:  0.0827610120177269
0  loss:  0.12897194921970367
0  loss:  0.15872256457805634
0  loss:  0.054245125502347946
0  loss:  0.1433156579732895
0  loss:  0.11732037365436554
0  loss:  0.03937607631087303
0  loss:  0.13007852435112
0  loss:  0.24274995923042297
0  loss:  0.11637695878744125
0  loss:  0.14959324896335602
0  loss:  0.17163079977035522
0  loss:  0.1296595185995102
------ 全部罪名 ------
Acc:  0.8168773488213188
mi_f1:  0.6031195840554592
ma_f1:  0.5882187841333666
mi_precision:  0.7446504992867332
mi_recall:  0.5067961165048543
------ 交通肇事 ------
Acc:  0.9047619047619048
mi_f1:  0.7942238267148015
ma_f1:  0.7944372753196283
mi_precision:  0.7913669064748201
mi_recall:  0.7971014492753623
------ 抢劫 ------
Acc:  0.7645788336933045
mi_f1:  0.5723905723905724
ma_f1:  0.5107202988529939
mi_precision:  0.7391304347826086
mi_recall:  0.46703296703296704
------ 抢夺 ------
Acc:  0.802439024390244
mi_f1:  0.49019607843137253
ma_f1:  0.47688575137942224
mi_precision:  0.7142857142857143
mi_recall:  0.373134328358209
------ 过失致人死亡 ------
Acc:  0.8249336870026526
mi_f1:  0.6886446886446886
ma_f1:  0.6471915166969686
mi_precision:  0.734375
mi_recall:  0.6482758620689655
------ 贪污 ------
Acc:  0.790948275862069
mi_f1:  0.4533333333333333
ma_f1:  0.4500945673034348
mi_precision:  0.6891891891891891
mi_recall:  0.33774834437086093
------ 挪用公款 ------
Acc:  0.8130563798219584
mi_f1:  0.5658536585365854
ma_f1:  0.507303504800724
mi_precision:  0.8055555555555556
mi_recall:  0.43609022556390975
------ 挪用资金 ------
Acc:  0.8245614035087719
mi_f1:  0.5920000000000001
ma_f1:  0.5603053435114504
mi_precision:  0.7184466019417476
mi_recall:  0.5034013605442177
0   tensor(5.1634, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0160961151123047
0  loss:  0.3332306146621704
0  loss:  0.20976263284683228
0  loss:  0.2162802815437317
0  loss:  0.275982528924942
0  loss:  0.06789190322160721
0  loss:  0.14418382942676544
0  loss:  0.15908461809158325
0  loss:  0.15678413212299347
0  loss:  0.11589591950178146
0  loss:  0.1381223499774933
0  loss:  0.1279328465461731
0  loss:  0.12535828351974487
0  loss:  0.10980717092752457
0  loss:  0.14188730716705322
0  loss:  0.3163396716117859
0  loss:  0.2372685819864273
0  loss:  0.1782769113779068
0  loss:  0.10939434915781021
0  loss:  0.16142694652080536
0  loss:  0.265722393989563
0  loss:  0.061069052666425705
0  loss:  0.0968845933675766
0  loss:  0.15627031028270721
0  loss:  0.10583309084177017
0  loss:  0.1471591293811798
0  loss:  0.1357809454202652
0  loss:  0.08727108687162399
0  loss:  0.11239215731620789
0  loss:  0.11269006878137589
0  loss:  0.1640804260969162
0  loss:  0.1996120810508728
0  loss:  0.07481760531663895
0  loss:  0.06519532948732376
0  loss:  0.16971522569656372
0  loss:  0.251202791929245
0  loss:  0.13469219207763672
0  loss:  0.14368344843387604
0  loss:  0.12625694274902344
0  loss:  0.1363506019115448
0  loss:  0.1513729989528656
0  loss:  0.12964554131031036
0  loss:  0.07179383933544159
0  loss:  0.23046080768108368
0  loss:  0.12988193333148956
0  loss:  0.10742589831352234
0  loss:  0.11477213352918625
0  loss:  0.15708957612514496
0  loss:  0.11013238877058029
0  loss:  0.11872411519289017
0  loss:  0.11379136890172958
0  loss:  0.11986523121595383
0  loss:  0.16857919096946716
0  loss:  0.10091612488031387
0  loss:  0.09170772135257721
0  loss:  0.13158559799194336
0  loss:  0.08357905596494675
0  loss:  0.15013901889324188
0  loss:  0.17471174895763397
0  loss:  0.13069060444831848
0  loss:  0.1425534337759018
0  loss:  0.1828336864709854
0  loss:  0.10749711841344833
0  loss:  0.14344851672649384
0  loss:  0.23883210122585297
0  loss:  0.10106466710567474
0  loss:  0.09249439090490341
0  loss:  0.3013375401496887
0  loss:  0.12780404090881348
0  loss:  0.22071623802185059
0  loss:  0.15058690309524536
0  loss:  0.13969726860523224
0  loss:  0.2539767324924469
0  loss:  0.09305775910615921
0  loss:  0.16264016926288605
0  loss:  0.11776158958673477
0  loss:  0.08008412271738052
0  loss:  0.19815750420093536
0  loss:  0.06591654568910599
0  loss:  0.1479259580373764
0  loss:  0.24492047727108002
0  loss:  0.1275930404663086
0  loss:  0.13649411499500275
0  loss:  0.2619321048259735
0  loss:  0.2093438357114792
0  loss:  0.08885816484689713
0  loss:  0.03466920182108879
0  loss:  0.07213038951158524
0  loss:  0.1347065269947052
0  loss:  0.2612171769142151
0  loss:  0.21263845264911652
0  loss:  0.08130217343568802
0  loss:  0.18576334416866302
0  loss:  0.18143734335899353
0  loss:  0.11065791547298431
0  loss:  0.1154637336730957
0  loss:  0.32759252190589905
0  loss:  0.11002442240715027
0  loss:  0.1505083292722702
0  loss:  0.20781119167804718
------ 全部罪名 ------
Acc:  0.8009759498082956
mi_f1:  0.6459500764136525
ma_f1:  0.6346282201107457
mi_precision:  0.728735632183908
mi_recall:  0.5800548947849954
------ 交通肇事 ------
Acc:  0.9157894736842105
mi_f1:  0.8562874251497006
ma_f1:  0.8428755587696677
mi_precision:  0.8614457831325302
mi_recall:  0.8511904761904762
------ 抢劫 ------
Acc:  0.733502538071066
mi_f1:  0.5543859649122808
ma_f1:  0.47781535929061053
mi_precision:  0.6752136752136753
mi_recall:  0.47023809523809523
------ 抢夺 ------
Acc:  0.78
mi_f1:  0.619718309859155
ma_f1:  0.6263570048672789
mi_precision:  0.6153846153846154
mi_recall:  0.624113475177305
------ 过失致人死亡 ------
Acc:  0.791005291005291
mi_f1:  0.6878980891719746
ma_f1:  0.6746509209744505
mi_precision:  0.7448275862068966
mi_recall:  0.6390532544378699
------ 贪污 ------
Acc:  0.7530266343825666
mi_f1:  0.5220883534136547
ma_f1:  0.4895502645502646
mi_precision:  0.6310679611650486
mi_recall:  0.4452054794520548
------ 挪用公款 ------
Acc:  0.8115577889447236
mi_f1:  0.6370967741935484
ma_f1:  0.5503310857201671
mi_precision:  0.8494623655913979
mi_recall:  0.5096774193548387
------ 挪用资金 ------
Acc:  0.8004866180048662
mi_f1:  0.5783132530120483
ma_f1:  0.5632282168186423
mi_precision:  0.6990291262135923
mi_recall:  0.4931506849315068
0   tensor(5.1401, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0888121128082275
0  loss:  0.39291608333587646
0  loss:  0.11918004602193832
0  loss:  0.11501270532608032
0  loss:  0.15084081888198853
0  loss:  0.18546192348003387
0  loss:  0.19871143996715546
0  loss:  0.17810992896556854
0  loss:  0.07177699357271194
0  loss:  0.1980466991662979
0  loss:  0.1181429997086525
0  loss:  0.038674429059028625
0  loss:  0.06356650590896606
0  loss:  0.1962372064590454
0  loss:  0.2509736716747284
0  loss:  0.09268223494291306
0  loss:  0.18635530769824982
0  loss:  0.07399558275938034
0  loss:  0.08285203576087952
0  loss:  0.2058050036430359
0  loss:  0.23357774317264557
0  loss:  0.09961175918579102
0  loss:  0.07047930359840393
0  loss:  0.21925170719623566
0  loss:  0.22642181813716888
0  loss:  0.18487782776355743
0  loss:  0.10139984637498856
0  loss:  0.10148493200540543
0  loss:  0.10135096311569214
0  loss:  0.2729708254337311
0  loss:  0.09431987255811691
0  loss:  0.4210071265697479
0  loss:  0.09297650307416916
0  loss:  0.09964413195848465
0  loss:  0.15340155363082886
0  loss:  0.17045223712921143
0  loss:  0.24355344474315643
0  loss:  0.045238375663757324
0  loss:  0.14015652239322662
0  loss:  0.14133761823177338
0  loss:  0.08031175285577774
0  loss:  0.05462523549795151
0  loss:  0.07664643973112106
0  loss:  0.14236485958099365
0  loss:  0.14140763878822327
0  loss:  0.09539996832609177
0  loss:  0.06935128569602966
0  loss:  0.19185183942317963
0  loss:  0.07703763991594315
0  loss:  0.02697847969830036
0  loss:  0.19271720945835114
0  loss:  0.15054157376289368
0  loss:  0.12214340269565582
0  loss:  0.11933986097574234
0  loss:  0.21380765736103058
0  loss:  0.057659462094306946
0  loss:  0.1852768212556839
0  loss:  0.07551506906747818
0  loss:  0.24596302211284637
0  loss:  0.1677289605140686
0  loss:  0.15683378279209137
0  loss:  0.09958507120609283
0  loss:  0.15450789034366608
0  loss:  0.09022577852010727
0  loss:  0.10758357495069504
0  loss:  0.09109003096818924
0  loss:  0.10061770677566528
0  loss:  0.289791464805603
0  loss:  0.14321620762348175
0  loss:  0.14490433037281036
0  loss:  0.12832584977149963
0  loss:  0.2631062865257263
0  loss:  0.0645580068230629
0  loss:  0.2399025410413742
0  loss:  0.08487559854984283
0  loss:  0.0814012661576271
0  loss:  0.15925543010234833
0  loss:  0.24060478806495667
0  loss:  0.17721955478191376
0  loss:  0.15942998230457306
0  loss:  0.09928681701421738
0  loss:  0.14753365516662598
0  loss:  0.10953354090452194
0  loss:  0.24736297130584717
0  loss:  0.2770228385925293
0  loss:  0.12657921016216278
0  loss:  0.15080641210079193
0  loss:  0.05392787605524063
0  loss:  0.12569734454154968
0  loss:  0.0859307274222374
0  loss:  0.17049960792064667
0  loss:  0.09443103522062302
0  loss:  0.0887647494673729
0  loss:  0.12389057129621506
0  loss:  0.09918913245201111
0  loss:  0.2282198816537857
0  loss:  0.14261887967586517
0  loss:  0.10825472325086594
0  loss:  0.05684047192335129
0  loss:  0.2235156148672104
------ 全部罪名 ------
Acc:  0.8085260115606936
mi_f1:  0.6395778364116095
ma_f1:  0.6311059441161719
mi_precision:  0.7546699875466999
mi_recall:  0.554945054945055
------ 交通肇事 ------
Acc:  0.8969072164948454
mi_f1:  0.8115942028985508
ma_f1:  0.7952081461515423
mi_precision:  0.8175182481751825
mi_recall:  0.8057553956834532
------ 抢劫 ------
Acc:  0.7659090909090909
mi_f1:  0.5540540540540542
ma_f1:  0.5399324968816495
mi_precision:  0.6356589147286822
mi_recall:  0.49101796407185627
------ 抢夺 ------
Acc:  0.8035264483627204
mi_f1:  0.5809128630705395
ma_f1:  0.583025841585982
mi_precision:  0.7865168539325843
mi_recall:  0.4605263157894737
------ 过失致人死亡 ------
Acc:  0.8270142180094787
mi_f1:  0.6996699669966996
ma_f1:  0.6608369119518223
mi_precision:  0.7412587412587412
mi_recall:  0.6625
------ 贪污 ------
Acc:  0.7734806629834254
mi_f1:  0.5747126436781609
ma_f1:  0.5539557544945476
mi_precision:  0.7894736842105263
mi_recall:  0.45180722891566266
------ 挪用公款 ------
Acc:  0.7877984084880637
mi_f1:  0.5976095617529881
ma_f1:  0.579401544097992
mi_precision:  0.7978723404255319
mi_recall:  0.47770700636942676
------ 挪用资金 ------
Acc:  0.806282722513089
mi_f1:  0.6441947565543071
ma_f1:  0.6144092808183536
mi_precision:  0.7413793103448276
mi_recall:  0.5695364238410596
0   tensor(5.1694, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0342687368392944
0  loss:  0.2776615023612976
0  loss:  0.13367792963981628
0  loss:  0.1560983508825302
0  loss:  0.17956173419952393
0  loss:  0.07930359244346619
0  loss:  0.12584595382213593
0  loss:  0.09715104103088379
0  loss:  0.10541194677352905
0  loss:  0.05797082558274269
0  loss:  0.1604890078306198
0  loss:  0.2208286076784134
0  loss:  0.11450131982564926
0  loss:  0.16217927634716034
0  loss:  0.1880321353673935
0  loss:  0.12601986527442932
0  loss:  0.2334088534116745
0  loss:  0.12287387251853943
0  loss:  0.09711714833974838
0  loss:  0.1821923553943634
0  loss:  0.07481635361909866
0  loss:  0.24414949119091034
0  loss:  0.16653761267662048
0  loss:  0.1492127627134323
0  loss:  0.10656853765249252
0  loss:  0.1017967164516449
0  loss:  0.0905785858631134
0  loss:  0.07151354849338531
0  loss:  0.08764023333787918
0  loss:  0.1494118869304657
0  loss:  0.1719423234462738
0  loss:  0.04139835014939308
0  loss:  0.08663472533226013
0  loss:  0.24705244600772858
0  loss:  0.14159341156482697
0  loss:  0.06153387203812599
0  loss:  0.17024625837802887
0  loss:  0.10867191851139069
0  loss:  0.0909402072429657
0  loss:  0.0520583875477314
0  loss:  0.1424487680196762
0  loss:  0.15036450326442719
0  loss:  0.09963395446538925
0  loss:  0.07572939991950989
0  loss:  0.11083085834980011
0  loss:  0.08659230917692184
0  loss:  0.04984746500849724
0  loss:  0.10778990387916565
0  loss:  0.16750256717205048
0  loss:  0.12482486665248871
0  loss:  0.10530967265367508
0  loss:  0.13270965218544006
0  loss:  0.24324654042720795
0  loss:  0.08086177706718445
0  loss:  0.12531369924545288
0  loss:  0.13750062882900238
0  loss:  0.12686726450920105
0  loss:  0.1420910507440567
0  loss:  0.06660431623458862
0  loss:  0.1511661857366562
0  loss:  0.10168145596981049
0  loss:  0.026459934189915657
0  loss:  0.0939052402973175
0  loss:  0.1370692402124405
0  loss:  0.12216596305370331
0  loss:  0.14526699483394623
0  loss:  0.1566273421049118
0  loss:  0.25908681750297546
0  loss:  0.06542068719863892
0  loss:  0.05145064368844032
0  loss:  0.06310774385929108
0  loss:  0.09135676920413971
0  loss:  0.216293066740036
0  loss:  0.19381974637508392
0  loss:  0.1705148071050644
0  loss:  0.17779941856861115
0  loss:  0.03307830169796944
0  loss:  0.05348953604698181
0  loss:  0.16237394511699677
0  loss:  0.07874860614538193
0  loss:  0.1118541955947876
0  loss:  0.1779928356409073
0  loss:  0.16490937769412994
0  loss:  0.04465958848595619
0  loss:  0.16435810923576355
0  loss:  0.14568348228931427
0  loss:  0.18881750106811523
0  loss:  0.1950397789478302
0  loss:  0.13375288248062134
0  loss:  0.1258280873298645
0  loss:  0.09041489660739899
0  loss:  0.14683984220027924
0  loss:  0.12851104140281677
0  loss:  0.12719832360744476
0  loss:  0.0860942006111145
0  loss:  0.18983931839466095
0  loss:  0.19732961058616638
0  loss:  0.19341455399990082
0  loss:  0.036966722458601
0  loss:  0.10735379904508591
------ 全部罪名 ------
Acc:  0.7855989667420084
mi_f1:  0.61941309255079
ma_f1:  0.6133022148404343
mi_precision:  0.7305644302449414
mi_recall:  0.5376175548589341
------ 交通肇事 ------
Acc:  0.870026525198939
mi_f1:  0.7936507936507937
ma_f1:  0.774201680672269
mi_precision:  0.8503401360544217
mi_recall:  0.7440476190476191
------ 抢劫 ------
Acc:  0.7450110864745011
mi_f1:  0.5679758308157099
ma_f1:  0.5333163107449294
mi_precision:  0.6666666666666666
mi_recall:  0.49473684210526314
------ 抢夺 ------
Acc:  0.8232758620689655
mi_f1:  0.5910652920962199
ma_f1:  0.6045328399629972
mi_precision:  0.6666666666666666
mi_recall:  0.5308641975308642
------ 过失致人死亡 ------
Acc:  0.8140096618357487
mi_f1:  0.6799999999999999
ma_f1:  0.6758443549997091
mi_precision:  0.7786259541984732
mi_recall:  0.6035502958579881
------ 贪污 ------
Acc:  0.7156862745098039
mi_f1:  0.5386666666666666
ma_f1:  0.5079285011636328
mi_precision:  0.5674157303370787
mi_recall:  0.5126903553299492
------ 挪用公款 ------
Acc:  0.7534791252485089
mi_f1:  0.5123456790123457
ma_f1:  0.49236981085728326
mi_precision:  0.7685185185185185
mi_recall:  0.38425925925925924
------ 挪用资金 ------
Acc:  0.8095238095238095
mi_f1:  0.6810035842293907
ma_f1:  0.5897891085627738
mi_precision:  0.9047619047619048
mi_recall:  0.5459770114942529
0   tensor(5.0627, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
0.6203750044842404
