Some weights of the model checkpoint at ../../../RESOURCE/lawformer were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerModel were not initialized from the model checkpoint at ../../../RESOURCE/lawformer and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
start loading
end loading
681
681
133
133
133
133
149
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  0.8718889951705933
0  loss:  0.3028993010520935
0  loss:  0.19415122270584106
0  loss:  0.06996236741542816
0  loss:  0.08984106779098511
0  loss:  0.0971994400024414
0  loss:  0.09090479463338852
0  loss:  0.1511376053094864
0  loss:  0.1315934956073761
0  loss:  0.15265138447284698
0  loss:  0.09623220562934875
0  loss:  0.15874172747135162
0  loss:  0.12982682883739471
0  loss:  0.0966869369149208
0  loss:  0.10435289144515991
0  loss:  0.10811521112918854
0  loss:  0.08953846246004105
0  loss:  0.06265959143638611
0  loss:  0.07096803188323975
0  loss:  0.21054673194885254
0  loss:  0.11154189705848694
0  loss:  0.11934427917003632
0  loss:  0.14100727438926697
0  loss:  0.11216634511947632
0  loss:  0.16162188351154327
0  loss:  0.1037413626909256
0  loss:  0.11347535997629166
0  loss:  0.18407376110553741
0  loss:  0.09652937203645706
0  loss:  0.1388576775789261
0  loss:  0.06238428130745888
0  loss:  0.11554588377475739
0  loss:  0.08361141383647919
0  loss:  0.05423024296760559
0  loss:  0.15961797535419464
0  loss:  0.06454077363014221
0  loss:  0.12439458817243576
0  loss:  0.17399805784225464
0  loss:  0.16461126506328583
0  loss:  0.053616560995578766
0  loss:  0.10160502791404724
0  loss:  0.12222333252429962
0  loss:  0.13410180807113647
0  loss:  0.09644318372011185
0  loss:  0.10842010378837585
0  loss:  0.20057980716228485
0  loss:  0.10115212202072144
0  loss:  0.12809135019779205
0  loss:  0.16305427253246307
0  loss:  0.13039632141590118
0  loss:  0.04013757407665253
0  loss:  0.150311678647995
0  loss:  0.08536234498023987
0  loss:  0.11111173778772354
0  loss:  0.21826373040676117
0  loss:  0.12765277922153473
0  loss:  0.08593156933784485
0  loss:  0.1146819069981575
0  loss:  0.07504964619874954
0  loss:  0.14569799602031708
0  loss:  0.11367259919643402
0  loss:  0.023759029805660248
0  loss:  0.06269854307174683
0  loss:  0.11685843020677567
0  loss:  0.027340024709701538
0  loss:  0.13185682892799377
0  loss:  0.1877109259366989
0  loss:  0.08382752537727356
0  loss:  0.0471624992787838
0  loss:  0.19121481478214264
0  loss:  0.16331838071346283
0  loss:  0.21593642234802246
0  loss:  0.10435274243354797
0  loss:  0.1885850876569748
0  loss:  0.18791881203651428
0  loss:  0.11479734629392624
0  loss:  0.1779361218214035
0  loss:  0.0806412547826767
0  loss:  0.18436197936534882
0  loss:  0.1850845217704773
0  loss:  0.13247184455394745
0  loss:  0.1532585620880127
0  loss:  0.04655945301055908
0  loss:  0.2540041208267212
0  loss:  0.12268922477960587
0  loss:  0.09798460453748703
0  loss:  0.13402023911476135
0  loss:  0.12230420857667923
0  loss:  0.15094777941703796
0  loss:  0.2287839949131012
0  loss:  0.19153755903244019
0  loss:  0.15491120517253876
0  loss:  0.09972214698791504
0  loss:  0.11622515320777893
0  loss:  0.2011466771364212
0  loss:  0.19211019575595856
0  loss:  0.17204971611499786
0  loss:  0.15920601785182953
0  loss:  0.13078303635120392
0  loss:  0.22879105806350708
------ 全部罪名 ------
Acc:  0.7985046728971963
mi_f1:  0.6371951219512195
ma_f1:  0.6311620177856175
mi_precision:  0.7446555819477435
mi_recall:  0.5568383658969804
------ 交通肇事 ------
Acc:  0.8866498740554156
mi_f1:  0.8250825082508252
ma_f1:  0.8281209952485165
mi_precision:  0.8223684210526315
mi_recall:  0.8278145695364238
------ 抢劫 ------
Acc:  0.75
mi_f1:  0.6098360655737706
ma_f1:  0.5699218512619199
mi_precision:  0.75
mi_recall:  0.5138121546961326
------ 抢夺 ------
Acc:  0.7849740932642487
mi_f1:  0.5519713261648745
ma_f1:  0.5612278738133731
mi_precision:  0.7
mi_recall:  0.4556213017751479
------ 过失致人死亡 ------
Acc:  0.8351063829787234
mi_f1:  0.6923076923076922
ma_f1:  0.6891970091522378
mi_precision:  0.7258064516129032
mi_recall:  0.6617647058823529
------ 贪污 ------
Acc:  0.7624434389140271
mi_f1:  0.4824902723735408
ma_f1:  0.5004683840749414
mi_precision:  0.5794392523364486
mi_recall:  0.41333333333333333
------ 挪用公款 ------
Acc:  0.7592067988668555
mi_f1:  0.5393258426966292
ma_f1:  0.49991185285302936
mi_precision:  0.7128712871287128
mi_recall:  0.43373493975903615
------ 挪用资金 ------
Acc:  0.8108882521489972
mi_f1:  0.7272727272727273
ma_f1:  0.7104523633846181
mi_precision:  0.8709677419354839
mi_recall:  0.6242774566473989
0   tensor(5.0826, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0710664987564087
0  loss:  0.3954773545265198
0  loss:  0.054551057517528534
0  loss:  0.23243683576583862
0  loss:  0.16044552624225616
0  loss:  0.14512404799461365
0  loss:  0.0691739171743393
0  loss:  0.19700968265533447
0  loss:  0.1142602264881134
0  loss:  0.12141866236925125
0  loss:  0.058796416968107224
0  loss:  0.09386207163333893
0  loss:  0.05686412379145622
0  loss:  0.10286077111959457
0  loss:  0.0823083370923996
0  loss:  0.10864470899105072
0  loss:  0.15810124576091766
0  loss:  0.20687933266162872
0  loss:  0.2263195812702179
0  loss:  0.20714102685451508
0  loss:  0.17074955999851227
0  loss:  0.1543925255537033
0  loss:  0.20374654233455658
0  loss:  0.36779719591140747
0  loss:  0.11934904754161835
0  loss:  0.068650022149086
0  loss:  0.035643257200717926
0  loss:  0.15625052154064178
0  loss:  0.2027411311864853
0  loss:  0.031975217163562775
0  loss:  0.08742179721593857
0  loss:  0.1367286741733551
0  loss:  0.0901232659816742
0  loss:  0.07645542919635773
0  loss:  0.06615332514047623
0  loss:  0.11625063419342041
0  loss:  0.16633202135562897
0  loss:  0.11103318631649017
0  loss:  0.1921788603067398
0  loss:  0.10188928246498108
0  loss:  0.1781785637140274
0  loss:  0.2030898779630661
0  loss:  0.0885755643248558
0  loss:  0.06253937631845474
0  loss:  0.17912299931049347
0  loss:  0.1812550276517868
0  loss:  0.11442089080810547
0  loss:  0.07893171906471252
0  loss:  0.2061481922864914
0  loss:  0.1309075802564621
0  loss:  0.06139156594872475
0  loss:  0.08221349120140076
0  loss:  0.08270532637834549
0  loss:  0.054518528282642365
0  loss:  0.05959627032279968
0  loss:  0.09488005191087723
0  loss:  0.0582345612347126
0  loss:  0.0587211474776268
0  loss:  0.048131003975868225
0  loss:  0.23456643521785736
0  loss:  0.04707339406013489
0  loss:  0.1777002215385437
0  loss:  0.07636183500289917
0  loss:  0.09819270670413971
0  loss:  0.13350699841976166
0  loss:  0.09562423825263977
0  loss:  0.0451507568359375
0  loss:  0.15699641406536102
0  loss:  0.08743731677532196
0  loss:  0.07799886167049408
0  loss:  0.14642085134983063
0  loss:  0.20037968456745148
0  loss:  0.09441589564085007
0  loss:  0.06479231268167496
0  loss:  0.20205248892307281
0  loss:  0.0979345291852951
0  loss:  0.12812641263008118
0  loss:  0.23005284368991852
0  loss:  0.04002479091286659
0  loss:  0.11255073547363281
0  loss:  0.14264647662639618
0  loss:  0.207856684923172
0  loss:  0.09043474495410919
0  loss:  0.15924471616744995
0  loss:  0.06584113091230392
0  loss:  0.16659842431545258
0  loss:  0.1470770388841629
0  loss:  0.07063561677932739
0  loss:  0.0798875093460083
0  loss:  0.07503732293844223
0  loss:  0.09137190878391266
0  loss:  0.1598605215549469
0  loss:  0.10036468505859375
0  loss:  0.1925165206193924
0  loss:  0.0942845493555069
0  loss:  0.1299392431974411
0  loss:  0.07364754378795624
0  loss:  0.10639189928770065
0  loss:  0.19391362369060516
0  loss:  0.22364158928394318
------ 全部罪名 ------
Acc:  0.8202938161940554
mi_f1:  0.6369080021774632
ma_f1:  0.6307075206818537
mi_precision:  0.724907063197026
mi_recall:  0.5679611650485437
------ 交通肇事 ------
Acc:  0.9071428571428571
mi_f1:  0.8100358422939068
ma_f1:  0.7998241341991342
mi_precision:  0.8014184397163121
mi_recall:  0.8188405797101449
------ 抢劫 ------
Acc:  0.7537796976241901
mi_f1:  0.5934718100890207
ma_f1:  0.580474467974468
mi_precision:  0.6451612903225806
mi_recall:  0.5494505494505495
------ 抢夺 ------
Acc:  0.8341463414634146
mi_f1:  0.6188340807174888
ma_f1:  0.6070276737411568
mi_precision:  0.7752808988764045
mi_recall:  0.5149253731343284
------ 过失致人死亡 ------
Acc:  0.8249336870026526
mi_f1:  0.7285223367697595
ma_f1:  0.7008649856767784
mi_precision:  0.726027397260274
mi_recall:  0.7310344827586207
------ 贪污 ------
Acc:  0.7974137931034483
mi_f1:  0.48484848484848486
ma_f1:  0.4544836116264687
mi_precision:  0.7
mi_recall:  0.3708609271523179
------ 挪用公款 ------
Acc:  0.7774480712166172
mi_f1:  0.5446428571428571
ma_f1:  0.4982548624062726
mi_precision:  0.6703296703296703
mi_recall:  0.45864661654135336
------ 挪用资金 ------
Acc:  0.8464912280701754
mi_f1:  0.6349206349206349
ma_f1:  0.6653728403728403
mi_precision:  0.7619047619047619
mi_recall:  0.54421768707483
0   tensor(5.0912, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.047889232635498
0  loss:  0.2947103977203369
0  loss:  0.07578324526548386
0  loss:  0.07659962773323059
0  loss:  0.07971877604722977
0  loss:  0.09654161334037781
0  loss:  0.12077151238918304
0  loss:  0.12645429372787476
0  loss:  0.09470324963331223
0  loss:  0.11025375127792358
0  loss:  0.17723524570465088
0  loss:  0.12483123689889908
0  loss:  0.1865607500076294
0  loss:  0.10895010828971863
0  loss:  0.1446828842163086
0  loss:  0.06479263305664062
0  loss:  0.0483601875603199
0  loss:  0.16532784700393677
0  loss:  0.11103271692991257
0  loss:  0.07662580162286758
0  loss:  0.060139693319797516
0  loss:  0.13602185249328613
0  loss:  0.09512078762054443
0  loss:  0.12598395347595215
0  loss:  0.15332382917404175
0  loss:  0.11964567750692368
0  loss:  0.14301903545856476
0  loss:  0.05428902059793472
0  loss:  0.11656221747398376
0  loss:  0.06123088300228119
0  loss:  0.06329705566167831
0  loss:  0.04671602323651314
0  loss:  0.06996291130781174
0  loss:  0.17483481764793396
0  loss:  0.07718753069639206
0  loss:  0.26817572116851807
0  loss:  0.209781676530838
0  loss:  0.13381555676460266
0  loss:  0.06998421251773834
0  loss:  0.35780346393585205
0  loss:  0.2058500349521637
0  loss:  0.1803043633699417
0  loss:  0.04564553499221802
0  loss:  0.12882018089294434
0  loss:  0.09164878726005554
0  loss:  0.12275885045528412
0  loss:  0.12741829454898834
0  loss:  0.08581596612930298
0  loss:  0.10252822190523148
0  loss:  0.04242362454533577
0  loss:  0.07742264866828918
0  loss:  0.18314768373966217
0  loss:  0.2685279846191406
0  loss:  0.08117210865020752
0  loss:  0.1440025120973587
0  loss:  0.057363398373126984
0  loss:  0.1467539668083191
0  loss:  0.15969695150852203
0  loss:  0.06691086292266846
0  loss:  0.11300021409988403
0  loss:  0.11407136917114258
0  loss:  0.10572236031293869
0  loss:  0.09736211597919464
0  loss:  0.08027403056621552
0  loss:  0.12277525663375854
0  loss:  0.114056795835495
0  loss:  0.06653487682342529
0  loss:  0.13757988810539246
0  loss:  0.11016834527254105
0  loss:  0.09804218262434006
0  loss:  0.2339422106742859
0  loss:  0.13179177045822144
0  loss:  0.16742148995399475
0  loss:  0.11968420445919037
0  loss:  0.20184367895126343
0  loss:  0.16204042732715607
0  loss:  0.2272419035434723
0  loss:  0.08935284614562988
0  loss:  0.21292494237422943
0  loss:  0.15285000205039978
0  loss:  0.09714853763580322
0  loss:  0.09213608503341675
0  loss:  0.1122761219739914
0  loss:  0.08231619000434875
0  loss:  0.11915656179189682
0  loss:  0.09192166477441788
0  loss:  0.06651721149682999
0  loss:  0.10822689533233643
0  loss:  0.1793370097875595
0  loss:  0.07561148703098297
0  loss:  0.04178805649280548
0  loss:  0.06766460835933685
0  loss:  0.1958967000246048
0  loss:  0.1574278175830841
0  loss:  0.15051573514938354
0  loss:  0.16796548664569855
0  loss:  0.1306549608707428
0  loss:  0.038010526448488235
0  loss:  0.13401558995246887
0  loss:  0.13049760460853577
------ 全部罪名 ------
Acc:  0.8082955733705124
mi_f1:  0.6663291139240508
ma_f1:  0.6662222036568337
mi_precision:  0.746031746031746
mi_recall:  0.6020128087831657
------ 交通肇事 ------
Acc:  0.9242105263157895
mi_f1:  0.8492307692307692
ma_f1:  0.855511536744093
mi_precision:  0.8789808917197452
mi_recall:  0.8214285714285714
------ 抢劫 ------
Acc:  0.7182741116751269
mi_f1:  0.5813148788927336
ma_f1:  0.5478468812511366
mi_precision:  0.6942148760330579
mi_recall:  0.5
------ 抢夺 ------
Acc:  0.8175
mi_f1:  0.6857142857142857
ma_f1:  0.6998803221260849
mi_precision:  0.6906474820143885
mi_recall:  0.6808510638297872
------ 过失致人死亡 ------
Acc:  0.798941798941799
mi_f1:  0.6666666666666666
ma_f1:  0.6693788214443952
mi_precision:  0.7734375
mi_recall:  0.5857988165680473
------ 贪污 ------
Acc:  0.7651331719128329
mi_f1:  0.5864661654135339
ma_f1:  0.57787816314981
mi_precision:  0.65
mi_recall:  0.5342465753424658
------ 挪用公款 ------
Acc:  0.8140703517587939
mi_f1:  0.648
ma_f1:  0.5944261808742409
mi_precision:  0.8526315789473684
mi_recall:  0.5225806451612903
------ 挪用资金 ------
Acc:  0.7980535279805353
mi_f1:  0.6119402985074627
ma_f1:  0.5977450445535553
mi_precision:  0.6721311475409836
mi_recall:  0.5616438356164384
0   tensor(5.0055, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.1126409769058228
0  loss:  0.4065324366092682
0  loss:  0.17160339653491974
0  loss:  0.16534145176410675
0  loss:  0.03609035536646843
0  loss:  0.11596524715423584
0  loss:  0.09299951046705246
0  loss:  0.08674395829439163
0  loss:  0.14746524393558502
0  loss:  0.11347799003124237
0  loss:  0.16951397061347961
0  loss:  0.06373106688261032
0  loss:  0.15213321149349213
0  loss:  0.17049604654312134
0  loss:  0.18729268014431
0  loss:  0.357867956161499
0  loss:  0.16875576972961426
0  loss:  0.02964917942881584
0  loss:  0.10738923400640488
0  loss:  0.023245850577950478
0  loss:  0.1638464331626892
0  loss:  0.1301221400499344
0  loss:  0.12708322703838348
0  loss:  0.0553661584854126
0  loss:  0.13810588419437408
0  loss:  0.14572489261627197
0  loss:  0.12688229978084564
0  loss:  0.15348778665065765
0  loss:  0.1543884426355362
0  loss:  0.1656413972377777
0  loss:  0.10552357882261276
0  loss:  0.03575221449136734
0  loss:  0.14633752405643463
0  loss:  0.09468451142311096
0  loss:  0.13592752814292908
0  loss:  0.05448022112250328
0  loss:  0.1968909353017807
0  loss:  0.09253387898206711
0  loss:  0.11009969562292099
0  loss:  0.11988217383623123
0  loss:  0.12482186406850815
0  loss:  0.1599474996328354
0  loss:  0.19128365814685822
0  loss:  0.10835015028715134
0  loss:  0.13669012486934662
0  loss:  0.11633483320474625
0  loss:  0.17442241311073303
0  loss:  0.213486909866333
0  loss:  0.11683356761932373
0  loss:  0.08002348244190216
0  loss:  0.07864778488874435
0  loss:  0.1317567527294159
0  loss:  0.05004216730594635
0  loss:  0.12951813638210297
0  loss:  0.12658028304576874
0  loss:  0.1601841002702713
0  loss:  0.10007258504629135
0  loss:  0.14691407978534698
0  loss:  0.12296409159898758
0  loss:  0.07115907967090607
0  loss:  0.08982162177562714
0  loss:  0.06949269026517868
0  loss:  0.10137835890054703
0  loss:  0.29213055968284607
0  loss:  0.09342994540929794
0  loss:  0.13700035214424133
0  loss:  0.10811071842908859
0  loss:  0.0922282263636589
0  loss:  0.18110357224941254
0  loss:  0.0808480754494667
0  loss:  0.038591958582401276
0  loss:  0.20449982583522797
0  loss:  0.13164779543876648
0  loss:  0.047608423978090286
0  loss:  0.06371910125017166
0  loss:  0.08270386606454849
0  loss:  0.2186833769083023
0  loss:  0.1919621080160141
0  loss:  0.18409988284111023
0  loss:  0.1089697852730751
0  loss:  0.06145871430635452
0  loss:  0.04199216514825821
0  loss:  0.04274135082960129
0  loss:  0.12371397763490677
0  loss:  0.1480521857738495
0  loss:  0.31015104055404663
0  loss:  0.1183316558599472
0  loss:  0.1961587518453598
0  loss:  0.29738175868988037
0  loss:  0.06712410598993301
0  loss:  0.2645169794559479
0  loss:  0.2595255374908447
0  loss:  0.14326660335063934
0  loss:  0.07562165707349777
0  loss:  0.1077214702963829
0  loss:  0.1085701659321785
0  loss:  0.11278269439935684
0  loss:  0.15498493611812592
0  loss:  0.09550133347511292
0  loss:  0.15966415405273438
------ 全部罪名 ------
Acc:  0.8168352601156069
mi_f1:  0.6588859416445624
ma_f1:  0.6493268445589618
mi_precision:  0.7831021437578815
mi_recall:  0.5686813186813187
------ 交通肇事 ------
Acc:  0.8891752577319587
mi_f1:  0.7470817120622568
ma_f1:  0.7468145434812102
mi_precision:  0.8135593220338984
mi_recall:  0.6906474820143885
------ 抢劫 ------
Acc:  0.8068181818181818
mi_f1:  0.587360594795539
ma_f1:  0.5992360813789385
mi_precision:  0.7745098039215687
mi_recall:  0.47305389221556887
------ 抢夺 ------
Acc:  0.8085642317380353
mi_f1:  0.6299212598425197
ma_f1:  0.6230191318649518
mi_precision:  0.7843137254901961
mi_recall:  0.5263157894736842
------ 过失致人死亡 ------
Acc:  0.8317535545023697
mi_f1:  0.6784452296819788
ma_f1:  0.6492667337507453
mi_precision:  0.7804878048780488
mi_recall:  0.6
------ 贪污 ------
Acc:  0.7679558011049724
mi_f1:  0.6711409395973154
ma_f1:  0.6359355612540326
mi_precision:  0.7575757575757576
mi_recall:  0.6024096385542169
------ 挪用公款 ------
Acc:  0.7692307692307693
mi_f1:  0.576923076923077
ma_f1:  0.5309002433090025
mi_precision:  0.7281553398058253
mi_recall:  0.47770700636942676
------ 挪用资金 ------
Acc:  0.8403141361256544
mi_f1:  0.7196969696969697
ma_f1:  0.704809147842321
mi_precision:  0.8407079646017699
mi_recall:  0.6291390728476821
0   tensor(5.0497, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.1037492752075195
0  loss:  0.26449519395828247
0  loss:  0.059323761612176895
0  loss:  0.11315034329891205
0  loss:  0.07175154238939285
0  loss:  0.06739138066768646
0  loss:  0.1618947684764862
0  loss:  0.16039903461933136
0  loss:  0.10237964987754822
0  loss:  0.08331192284822464
0  loss:  0.11565376073122025
0  loss:  0.0737132802605629
0  loss:  0.291812926530838
0  loss:  0.21528996527194977
0  loss:  0.1395953744649887
0  loss:  0.21883268654346466
0  loss:  0.09335619956254959
0  loss:  0.07312608510255814
0  loss:  0.12651784718036652
0  loss:  0.2648797333240509
0  loss:  0.11211127787828445
0  loss:  0.07857862859964371
0  loss:  0.15495158731937408
0  loss:  0.15335117280483246
0  loss:  0.1639270782470703
0  loss:  0.06795646250247955
0  loss:  0.11311005800962448
0  loss:  0.09015310555696487
0  loss:  0.08055178821086884
0  loss:  0.07984114438295364
0  loss:  0.24413545429706573
0  loss:  0.112667016685009
0  loss:  0.2479143887758255
0  loss:  0.022638440132141113
0  loss:  0.11456955224275589
0  loss:  0.2136240452528
0  loss:  0.1533413827419281
0  loss:  0.17883725464344025
0  loss:  0.22126807272434235
0  loss:  0.24079155921936035
0  loss:  0.06534449756145477
0  loss:  0.0492868609726429
0  loss:  0.18167661130428314
0  loss:  0.06863417476415634
0  loss:  0.2793797254562378
0  loss:  0.19868500530719757
0  loss:  0.08995232731103897
0  loss:  0.17809449136257172
0  loss:  0.09336378425359726
0  loss:  0.1660565882921219
0  loss:  0.09503591060638428
0  loss:  0.09551896899938583
0  loss:  0.09934847801923752
0  loss:  0.08950266987085342
0  loss:  0.16022814810276031
0  loss:  0.1232471689581871
0  loss:  0.08873747289180756
0  loss:  0.21475669741630554
0  loss:  0.09938259422779083
0  loss:  0.13913555443286896
0  loss:  0.16273799538612366
0  loss:  0.07660333067178726
0  loss:  0.08604644984006882
0  loss:  0.07619524002075195
0  loss:  0.15426570177078247
0  loss:  0.14333878457546234
0  loss:  0.12694773077964783
0  loss:  0.14733628928661346
0  loss:  0.2540704309940338
0  loss:  0.12711331248283386
0  loss:  0.1482463926076889
0  loss:  0.1035405769944191
0  loss:  0.1221180111169815
0  loss:  0.08323099464178085
0  loss:  0.2568330764770508
0  loss:  0.10100438445806503
0  loss:  0.1295090764760971
0  loss:  0.05656394734978676
0  loss:  0.13237376511096954
0  loss:  0.0557359978556633
0  loss:  0.04702720418572426
0  loss:  0.12152712792158127
0  loss:  0.09636684507131577
0  loss:  0.09935008734464645
0  loss:  0.1379643976688385
0  loss:  0.15397091209888458
0  loss:  0.1206279918551445
0  loss:  0.06506244093179703
0  loss:  0.0733027383685112
0  loss:  0.05205405130982399
0  loss:  0.08661866933107376
0  loss:  0.15778426826000214
0  loss:  0.11482911556959152
0  loss:  0.19205814599990845
0  loss:  0.09704038500785828
0  loss:  0.1104656532406807
0  loss:  0.15830352902412415
0  loss:  0.16373981535434723
0  loss:  0.252774715423584
0  loss:  0.09172113984823227
------ 全部罪名 ------
Acc:  0.7978689053923151
mi_f1:  0.6357012750455374
ma_f1:  0.6331299987910255
mi_precision:  0.758695652173913
mi_recall:  0.5470219435736677
------ 交通肇事 ------
Acc:  0.8753315649867374
mi_f1:  0.8076923076923077
ma_f1:  0.8088453827928952
mi_precision:  0.875
mi_recall:  0.75
------ 抢劫 ------
Acc:  0.753880266075388
mi_f1:  0.5714285714285715
ma_f1:  0.5567030841498927
mi_precision:  0.6762589928057554
mi_recall:  0.49473684210526314
------ 抢夺 ------
Acc:  0.8146551724137931
mi_f1:  0.5611510791366906
ma_f1:  0.5558070321151716
mi_precision:  0.6724137931034483
mi_recall:  0.48148148148148145
------ 过失致人死亡 ------
Acc:  0.8309178743961353
mi_f1:  0.7306501547987616
ma_f1:  0.7165859066414109
mi_precision:  0.7662337662337663
mi_recall:  0.6982248520710059
------ 贪污 ------
Acc:  0.7529411764705882
mi_f1:  0.4838709677419355
ma_f1:  0.48401839339339336
mi_precision:  0.6637168141592921
mi_recall:  0.38071065989847713
------ 挪用公款 ------
Acc:  0.7554671968190855
mi_f1:  0.5513196480938417
ma_f1:  0.5329721362229103
mi_precision:  0.752
mi_recall:  0.4351851851851852
------ 挪用资金 ------
Acc:  0.8333333333333334
mi_f1:  0.7458745874587459
ma_f1:  0.6959017996870109
mi_precision:  0.875968992248062
mi_recall:  0.6494252873563219
0   tensor(4.9875, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
0.6470038909485667
