Some weights of the model checkpoint at ../../../RESOURCE/chinese_wwm_ext_pytorch were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
start loading
end loading
681
681
133
133
133
133
149
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.1330517530441284
0  loss:  0.5307083129882812
0  loss:  0.13370655477046967
0  loss:  0.07178202271461487
0  loss:  0.061167970299720764
0  loss:  0.10068491101264954
0  loss:  0.1266571432352066
0  loss:  0.04353467747569084
0  loss:  0.0038348634261637926
0  loss:  0.06182439252734184
0  loss:  0.06174579635262489
0  loss:  0.009815861470997334
0  loss:  0.03378469869494438
0  loss:  0.09215690195560455
0  loss:  0.05219189450144768
0  loss:  0.1593133956193924
0  loss:  0.052481066435575485
0  loss:  0.04591764137148857
0  loss:  0.04815216735005379
0  loss:  0.10508984327316284
0  loss:  0.032494381070137024
0  loss:  0.04565909132361412
0  loss:  0.1287790983915329
0  loss:  0.018103506416082382
0  loss:  0.05640862509608269
0  loss:  0.008168571628630161
0  loss:  0.036503229290246964
0  loss:  0.06452852487564087
0  loss:  0.04475142061710358
0  loss:  0.05928635969758034
0  loss:  0.062086377292871475
0  loss:  0.05508517846465111
0  loss:  0.15692035853862762
0  loss:  0.06864292919635773
0  loss:  0.029093977063894272
0  loss:  0.054421812295913696
0  loss:  0.030300166457891464
0  loss:  0.019119609147310257
0  loss:  0.07720960676670074
0  loss:  0.05379379913210869
0  loss:  0.01228985097259283
0  loss:  0.036592934280633926
0  loss:  0.033123329281806946
0  loss:  0.11307564377784729
0  loss:  0.061165645718574524
0  loss:  0.06482118368148804
0  loss:  0.12259002029895782
0  loss:  0.013763201422989368
0  loss:  0.03808975964784622
0  loss:  0.030250022187829018
0  loss:  0.06394768506288528
0  loss:  0.01547063048928976
0  loss:  0.20570559799671173
0  loss:  0.00795503705739975
0  loss:  0.027141472324728966
0  loss:  0.20395736396312714
0  loss:  0.12312796711921692
0  loss:  0.12010511010885239
0  loss:  0.007793931290507317
0  loss:  0.12009503692388535
0  loss:  0.07838819921016693
0  loss:  0.08044514805078506
0  loss:  0.10845789313316345
0  loss:  0.0569256916642189
0  loss:  0.09007886797189713
0  loss:  0.05715225264430046
0  loss:  0.01593373343348503
0  loss:  0.0060956962406635284
0  loss:  0.07146721333265305
0  loss:  0.10996164381504059
0  loss:  0.05636993423104286
0  loss:  0.05059831216931343
0  loss:  0.03575049713253975
0  loss:  0.05420686677098274
0  loss:  0.07448197901248932
0  loss:  0.1430657058954239
0  loss:  0.06639385223388672
0  loss:  0.03317748382687569
0  loss:  0.039177559316158295
0  loss:  0.06292950361967087
0  loss:  0.022066188976168633
0  loss:  0.07889145612716675
0  loss:  0.029705693945288658
0  loss:  0.010320797562599182
0  loss:  0.029396947473287582
0  loss:  0.044877681881189346
0  loss:  0.021966438740491867
0  loss:  0.04332764074206352
0  loss:  0.11547008901834488
0  loss:  0.0349639430642128
0  loss:  0.037546198815107346
0  loss:  0.041815776377916336
0  loss:  0.053550977259874344
0  loss:  0.040684450417757034
0  loss:  0.05178140476346016
0  loss:  0.05497414618730545
0  loss:  0.06979402899742126
0  loss:  0.07687235623598099
0  loss:  0.13387978076934814
0  loss:  0.1761063188314438
------ 全部罪名 ------
Acc:  0.8280373831775701
mi_f1:  0.7203028868906768
ma_f1:  0.7215669366870163
mi_precision:  0.7710233029381965
mi_recall:  0.6758436944937833
------ 交通肇事 ------
Acc:  0.9370277078085643
mi_f1:  0.9038461538461539
ma_f1:  0.9122247031300188
mi_precision:  0.8757763975155279
mi_recall:  0.9337748344370861
------ 抢劫 ------
Acc:  0.7526881720430108
mi_f1:  0.6455696202531646
ma_f1:  0.6186470668894657
mi_precision:  0.7555555555555555
mi_recall:  0.56353591160221
------ 抢夺 ------
Acc:  0.7668393782383419
mi_f1:  0.607843137254902
ma_f1:  0.6112915894809487
mi_precision:  0.6788321167883211
mi_recall:  0.5502958579881657
------ 过失致人死亡 ------
Acc:  0.8590425531914894
mi_f1:  0.75
ma_f1:  0.7651275630554045
mi_precision:  0.7291666666666666
mi_recall:  0.7720588235294118
------ 贪污 ------
Acc:  0.830316742081448
mi_f1:  0.6441947565543072
ma_f1:  0.6491083313757733
mi_precision:  0.7350427350427351
mi_recall:  0.5733333333333334
------ 挪用公款 ------
Acc:  0.7818696883852692
mi_f1:  0.6710097719869709
ma_f1:  0.6356433772841602
mi_precision:  0.7304964539007093
mi_recall:  0.6204819277108434
------ 挪用资金 ------
Acc:  0.8624641833810889
mi_f1:  0.806153846153846
ma_f1:  0.8034224306888814
mi_precision:  0.8618421052631579
mi_recall:  0.7572254335260116
0   tensor(1.2684, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.2398, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.2621, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2739, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.3470, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.3660, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.4078, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.4938, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4901, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.5151, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5857, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.5462, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  0.9924339056015015
0  loss:  0.5280448198318481
0  loss:  0.4189959168434143
0  loss:  0.15511266887187958
0  loss:  0.20463839173316956
0  loss:  0.21700845658779144
0  loss:  0.04504823312163353
0  loss:  0.15171866118907928
0  loss:  0.12755970656871796
0  loss:  0.04980422556400299
0  loss:  0.035753242671489716
0  loss:  0.0468495637178421
0  loss:  0.055962324142456055
0  loss:  0.0027639407198876143
0  loss:  0.06507854163646698
0  loss:  0.014525876380503178
0  loss:  0.020186854526400566
0  loss:  0.034091901034116745
0  loss:  0.038642946630716324
0  loss:  0.07033868134021759
0  loss:  0.06526053696870804
0  loss:  0.021555470302700996
0  loss:  0.04354438558220863
0  loss:  0.031113842502236366
0  loss:  0.10786093026399612
0  loss:  0.056241825222969055
0  loss:  0.036644987761974335
0  loss:  0.09255761653184891
0  loss:  0.10771846026182175
0  loss:  0.08111018687486649
0  loss:  0.08060254901647568
0  loss:  0.15154044330120087
0  loss:  0.04954028129577637
0  loss:  0.05541909486055374
0  loss:  0.08748143166303635
0  loss:  0.055665481835603714
0  loss:  0.02021777257323265
0  loss:  0.017712397500872612
0  loss:  0.07418374717235565
0  loss:  0.0686449483036995
0  loss:  0.1040024533867836
0  loss:  0.07817051559686661
0  loss:  0.00958996918052435
0  loss:  0.013132788240909576
0  loss:  0.1624349057674408
0  loss:  0.0717243179678917
0  loss:  0.1601029485464096
0  loss:  0.08199108392000198
0  loss:  0.07212784886360168
0  loss:  0.0803108736872673
0  loss:  0.022854836657643318
0  loss:  0.008345801383256912
0  loss:  0.24439303576946259
0  loss:  0.028685910627245903
0  loss:  0.05096520856022835
0  loss:  0.007031503599137068
0  loss:  0.03228897973895073
0  loss:  0.13041120767593384
0  loss:  0.11940278857946396
0  loss:  0.09819816797971725
0  loss:  0.06881217658519745
0  loss:  0.03823639452457428
0  loss:  0.026519477367401123
0  loss:  0.0074772960506379604
0  loss:  0.046037085354328156
0  loss:  0.036119088530540466
0  loss:  0.06111587956547737
0  loss:  0.07775410264730453
0  loss:  0.05293310433626175
0  loss:  0.09643834084272385
0  loss:  0.11237119883298874
0  loss:  0.017847107723355293
0  loss:  0.05185786262154579
0  loss:  0.08940774947404861
0  loss:  0.07217313349246979
0  loss:  0.050905339419841766
0  loss:  0.055735234171152115
0  loss:  0.06435022503137589
0  loss:  0.0447605699300766
0  loss:  0.07355628162622452
0  loss:  0.04208322986960411
0  loss:  0.061649322509765625
0  loss:  0.03858925402164459
0  loss:  0.07132736593484879
0  loss:  0.10173151642084122
0  loss:  0.18328092992305756
0  loss:  0.08836745470762253
0  loss:  0.1366717368364334
0  loss:  0.07046061754226685
0  loss:  0.044775817543268204
0  loss:  0.04938863217830658
0  loss:  0.12157829850912094
0  loss:  0.09685739874839783
0  loss:  0.09232939779758453
0  loss:  0.0497308149933815
0  loss:  0.032104406505823135
0  loss:  0.0909930020570755
0  loss:  0.05171138793230057
0  loss:  0.06966723501682281
0  loss:  0.05901302769780159
------ 全部罪名 ------
Acc:  0.8476255551759481
mi_f1:  0.7388096409247418
ma_f1:  0.7412545204637466
mi_precision:  0.748753738783649
mi_recall:  0.7291262135922331
------ 交通肇事 ------
Acc:  0.9428571428571428
mi_f1:  0.8808664259927799
ma_f1:  0.8961034256566902
mi_precision:  0.8776978417266187
mi_recall:  0.8840579710144928
------ 抢劫 ------
Acc:  0.7732181425485961
mi_f1:  0.6684782608695652
ma_f1:  0.6823869513080482
mi_precision:  0.6612903225806451
mi_recall:  0.6758241758241759
------ 抢夺 ------
Acc:  0.8390243902439024
mi_f1:  0.7058823529411764
ma_f1:  0.714314906095963
mi_precision:  0.743801652892562
mi_recall:  0.6716417910447762
------ 过失致人死亡 ------
Acc:  0.8541114058355438
mi_f1:  0.78
ma_f1:  0.776617048618548
mi_precision:  0.7548387096774194
mi_recall:  0.8068965517241379
------ 贪污 ------
Acc:  0.8448275862068966
mi_f1:  0.697841726618705
ma_f1:  0.6983715559802516
mi_precision:  0.7637795275590551
mi_recall:  0.6423841059602649
------ 挪用公款 ------
Acc:  0.8130563798219584
mi_f1:  0.6984126984126984
ma_f1:  0.6728966254389983
mi_precision:  0.7394957983193278
mi_recall:  0.6616541353383458
------ 挪用资金 ------
Acc:  0.8662280701754386
mi_f1:  0.7524752475247524
ma_f1:  0.7502138525211548
mi_precision:  0.7307692307692307
mi_recall:  0.7755102040816326
0   tensor(1.2729, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.2338, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.2652, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2800, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.3303, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.3536, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.4078, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.5049, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4752, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.5012, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5757, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.5196, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0478333234786987
0  loss:  0.3577951490879059
0  loss:  0.37441951036453247
0  loss:  0.14612235128879547
0  loss:  0.10291986167430878
0  loss:  0.02561507187783718
0  loss:  0.01000730786472559
0  loss:  0.04110298678278923
0  loss:  0.08826093375682831
0  loss:  0.03489573299884796
0  loss:  0.13171274960041046
0  loss:  0.10303980857133865
0  loss:  0.06225057318806648
0  loss:  0.12836496531963348
0  loss:  0.10320448130369186
0  loss:  0.01694217510521412
0  loss:  0.022824395447969437
0  loss:  0.04884994775056839
0  loss:  0.0379156731069088
0  loss:  0.08069520443677902
0  loss:  0.05674215033650398
0  loss:  0.09745433181524277
0  loss:  0.03588585928082466
0  loss:  0.06002091243863106
0  loss:  0.023094417527318
0  loss:  0.04563752934336662
0  loss:  0.1288483738899231
0  loss:  0.033113256096839905
0  loss:  0.06195118650794029
0  loss:  0.10783381760120392
0  loss:  0.07931535691022873
0  loss:  0.03464620187878609
0  loss:  0.019154099747538567
0  loss:  0.09918401390314102
0  loss:  0.08774115890264511
0  loss:  0.04981940612196922
0  loss:  0.05924450606107712
0  loss:  0.18446657061576843
0  loss:  0.04792483523488045
0  loss:  0.04920288920402527
0  loss:  0.10598187893629074
0  loss:  0.11668266355991364
0  loss:  0.09828610718250275
0  loss:  0.1312531679868698
0  loss:  0.11873062700033188
0  loss:  0.24738413095474243
0  loss:  0.025565439835190773
0  loss:  0.02558346651494503
0  loss:  0.0650688111782074
0  loss:  0.0854465439915657
0  loss:  0.10304976254701614
0  loss:  0.038626495748758316
0  loss:  0.06423257291316986
0  loss:  0.1214362308382988
0  loss:  0.07149328291416168
0  loss:  0.04845137894153595
0  loss:  0.04350224882364273
0  loss:  0.02972443588078022
0  loss:  0.04305209219455719
0  loss:  0.05211477726697922
0  loss:  0.07163336127996445
0  loss:  0.0291683841496706
0  loss:  0.06529229134321213
0  loss:  0.04520006105303764
0  loss:  0.02850109525024891
0  loss:  0.11505036801099777
0  loss:  0.037454184144735336
0  loss:  0.050050701946020126
0  loss:  0.046162497252225876
0  loss:  0.07899168133735657
0  loss:  0.04117538779973984
0  loss:  0.0330924466252327
0  loss:  0.08620753139257431
0  loss:  0.04193413630127907
0  loss:  0.0754629373550415
0  loss:  0.07199933379888535
0  loss:  0.0662071704864502
0  loss:  0.08591476827859879
0  loss:  0.01893826760351658
0  loss:  0.10564149171113968
0  loss:  0.06618205457925797
0  loss:  0.06652940809726715
0  loss:  0.06696976721286774
0  loss:  0.037061210721731186
0  loss:  0.028493357822299004
0  loss:  0.10201180726289749
0  loss:  0.10979931801557541
0  loss:  0.15782517194747925
0  loss:  0.08290758728981018
0  loss:  0.07920309156179428
0  loss:  0.08831176906824112
0  loss:  0.07193837314844131
0  loss:  0.03846127167344093
0  loss:  0.06102660670876503
0  loss:  0.08157697319984436
0  loss:  0.04419206827878952
0  loss:  0.10248322784900665
0  loss:  0.038828637450933456
0  loss:  0.057246413081884384
0  loss:  0.2445906102657318
------ 全部罪名 ------
Acc:  0.8459393516904845
mi_f1:  0.7545195052331114
ma_f1:  0.762789613589102
mi_precision:  0.7859266600594648
mi_recall:  0.7255260750228728
------ 交通肇事 ------
Acc:  0.9305263157894736
mi_f1:  0.8809523809523809
ma_f1:  0.8944874254797464
mi_precision:  0.8809523809523809
mi_recall:  0.8809523809523809
------ 抢劫 ------
Acc:  0.8020304568527918
mi_f1:  0.7207792207792209
ma_f1:  0.7048735772668245
mi_precision:  0.7928571428571428
mi_recall:  0.6607142857142857
------ 抢夺 ------
Acc:  0.8425
mi_f1:  0.7310344827586207
ma_f1:  0.7487896825396826
mi_precision:  0.7114093959731543
mi_recall:  0.75177304964539
------ 过失致人死亡 ------
Acc:  0.828042328042328
mi_f1:  0.7381703470031545
ma_f1:  0.7644066856301316
mi_precision:  0.7905405405405406
mi_recall:  0.6923076923076923
------ 贪污 ------
Acc:  0.8159806295399515
mi_f1:  0.7266666666666666
ma_f1:  0.730698541224857
mi_precision:  0.7077922077922078
mi_recall:  0.7465753424657534
------ 挪用公款 ------
Acc:  0.8316582914572864
mi_f1:  0.743859649122807
ma_f1:  0.728107210626186
mi_precision:  0.8153846153846154
mi_recall:  0.6838709677419355
------ 挪用资金 ------
Acc:  0.8540145985401459
mi_f1:  0.7218045112781954
ma_f1:  0.7200779789223186
mi_precision:  0.8
mi_recall:  0.6575342465753424
0   tensor(1.2772, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.2269, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.2437, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2663, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.3084, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.3181, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.4050, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.4900, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4814, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.5183, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5521, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.5177, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.1093112230300903
0  loss:  0.3156118094921112
0  loss:  0.2186754047870636
0  loss:  0.1140633299946785
0  loss:  0.06605314463376999
0  loss:  0.07573633641004562
0  loss:  0.027446644380688667
0  loss:  0.0966745987534523
0  loss:  0.08845560997724533
0  loss:  0.026578467339277267
0  loss:  0.012377065606415272
0  loss:  0.10797039419412613
0  loss:  0.059086576104164124
0  loss:  0.06258563697338104
0  loss:  0.040933579206466675
0  loss:  0.0916222557425499
0  loss:  0.03563380986452103
0  loss:  0.09492408484220505
0  loss:  0.03546789288520813
0  loss:  0.06032006815075874
0  loss:  0.07009632140398026
0  loss:  0.04903937503695488
0  loss:  0.10191429406404495
0  loss:  0.007179640233516693
0  loss:  0.09816162288188934
0  loss:  0.017203839495778084
0  loss:  0.19047608971595764
0  loss:  0.15725509822368622
0  loss:  0.04202326759696007
0  loss:  0.06831247359514236
0  loss:  0.10037744790315628
0  loss:  0.1356956958770752
0  loss:  0.004037449136376381
0  loss:  0.011127278208732605
0  loss:  0.04606497287750244
0  loss:  0.011473394930362701
0  loss:  0.10620278865098953
0  loss:  0.07871583849191666
0  loss:  0.12570792436599731
0  loss:  0.07443168014287949
0  loss:  0.02554050274193287
0  loss:  0.05582883581519127
0  loss:  0.044524550437927246
0  loss:  0.08660681545734406
0  loss:  0.05324454978108406
0  loss:  0.05832093581557274
0  loss:  0.08750560134649277
0  loss:  0.03875351697206497
0  loss:  0.09478329867124557
0  loss:  0.10984436422586441
0  loss:  0.042451925575733185
0  loss:  0.03202035650610924
0  loss:  0.05261766538023949
0  loss:  0.04601925611495972
0  loss:  0.08760204911231995
0  loss:  0.03357260301709175
0  loss:  0.031959548592567444
0  loss:  0.046979136765003204
0  loss:  0.10011644661426544
0  loss:  0.08202173560857773
0  loss:  0.07089207321405411
0  loss:  0.046530768275260925
0  loss:  0.10900400578975677
0  loss:  0.027461184188723564
0  loss:  0.12492074072360992
0  loss:  0.024497492238879204
0  loss:  0.038843508809804916
0  loss:  0.05360661447048187
0  loss:  0.008884638547897339
0  loss:  0.040271662175655365
0  loss:  0.03917897492647171
0  loss:  0.02654685638844967
0  loss:  0.01044252049177885
0  loss:  0.11791635304689407
0  loss:  0.021579697728157043
0  loss:  0.08834268152713776
0  loss:  0.04451802745461464
0  loss:  0.056660063564777374
0  loss:  0.12028848379850388
0  loss:  0.06469114869832993
0  loss:  0.030021285638213158
0  loss:  0.044930145144462585
0  loss:  0.08406249433755875
0  loss:  0.05185001716017723
0  loss:  0.05208803713321686
0  loss:  0.1501823216676712
0  loss:  0.007635165937244892
0  loss:  0.12301953881978989
0  loss:  0.0987561047077179
0  loss:  0.030530309304594994
0  loss:  0.11527594178915024
0  loss:  0.03357825428247452
0  loss:  0.18628841638565063
0  loss:  0.06624159961938858
0  loss:  0.12815047800540924
0  loss:  0.02984042838215828
0  loss:  0.024762161076068878
0  loss:  0.049021609127521515
0  loss:  0.029794536530971527
0  loss:  0.0999826192855835
------ 全部罪名 ------
Acc:  0.8442919075144508
mi_f1:  0.7527289985761746
ma_f1:  0.7569443646116387
mi_precision:  0.7812807881773399
mi_recall:  0.7261904761904762
------ 交通肇事 ------
Acc:  0.9252577319587629
mi_f1:  0.8529411764705883
ma_f1:  0.8595806668713251
mi_precision:  0.8721804511278195
mi_recall:  0.8345323741007195
------ 抢劫 ------
Acc:  0.8159090909090909
mi_f1:  0.6967741935483871
ma_f1:  0.7124274645147102
mi_precision:  0.7552447552447552
mi_recall:  0.6467065868263473
------ 抢夺 ------
Acc:  0.8261964735516373
mi_f1:  0.7058823529411764
ma_f1:  0.7134068609655736
mi_precision:  0.7445255474452555
mi_recall:  0.6710526315789473
------ 过失致人死亡 ------
Acc:  0.8601895734597157
mi_f1:  0.76158940397351
ma_f1:  0.7613604369615407
mi_precision:  0.8098591549295775
mi_recall:  0.71875
------ 贪污 ------
Acc:  0.8259668508287292
mi_f1:  0.7662337662337662
ma_f1:  0.7656470700191005
mi_precision:  0.8309859154929577
mi_recall:  0.7108433734939759
------ 挪用公款 ------
Acc:  0.8222811671087533
mi_f1:  0.7555555555555556
ma_f1:  0.7288606660030186
mi_precision:  0.7531645569620253
mi_recall:  0.7579617834394905
------ 挪用资金 ------
Acc:  0.8350785340314136
mi_f1:  0.7395498392282958
ma_f1:  0.7450224992794652
mi_precision:  0.71875
mi_recall:  0.7615894039735099
0   tensor(1.2759, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.2326, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.2513, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2674, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.3369, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.3534, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.4200, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.5173, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4879, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.5085, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5956, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.5340, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  0.9605342745780945
0  loss:  0.3825554847717285
0  loss:  0.219239741563797
0  loss:  0.05862610414624214
0  loss:  0.061373867094516754
0  loss:  0.21570797264575958
0  loss:  0.043937698006629944
0  loss:  0.05501324310898781
0  loss:  0.045155905187129974
0  loss:  0.01598910614848137
0  loss:  0.13739842176437378
0  loss:  0.07984437793493271
0  loss:  0.03704562038183212
0  loss:  0.02348942682147026
0  loss:  0.016046978533267975
0  loss:  0.047804251313209534
0  loss:  0.09336128830909729
0  loss:  0.01647547259926796
0  loss:  0.10861282795667648
0  loss:  0.03455256298184395
0  loss:  0.08402445167303085
0  loss:  0.05751393362879753
0  loss:  0.07053948938846588
0  loss:  0.11467543989419937
0  loss:  0.12184561789035797
0  loss:  0.008829601109027863
0  loss:  0.05008377879858017
0  loss:  0.13172535598278046
0  loss:  0.09510919451713562
0  loss:  0.021850427612662315
0  loss:  0.0816827192902565
0  loss:  0.010291414335370064
0  loss:  0.15000437200069427
0  loss:  0.08776634186506271
0  loss:  0.034568168222904205
0  loss:  0.06082790717482567
0  loss:  0.06358960270881653
0  loss:  0.034120503813028336
0  loss:  0.011789863929152489
0  loss:  0.009406322613358498
0  loss:  0.001308692037127912
0  loss:  0.049585338681936264
0  loss:  0.04971441626548767
0  loss:  0.10490649193525314
0  loss:  0.11756189167499542
0  loss:  0.03173542022705078
0  loss:  0.06392806023359299
0  loss:  0.03140977770090103
0  loss:  0.03798473998904228
0  loss:  0.05635903403162956
0  loss:  0.06280145794153214
0  loss:  0.021340448409318924
0  loss:  0.05521577224135399
0  loss:  0.06632824242115021
0  loss:  0.0276231337338686
0  loss:  0.06527379900217056
0  loss:  0.07292228192090988
0  loss:  0.0882905125617981
0  loss:  0.038048963993787766
0  loss:  0.06382324546575546
0  loss:  0.036774247884750366
0  loss:  0.03676554188132286
0  loss:  0.0664977878332138
0  loss:  0.16416803002357483
0  loss:  0.022154955193400383
0  loss:  0.15105237066745758
0  loss:  0.02014627680182457
0  loss:  0.12831243872642517
0  loss:  0.0782611072063446
0  loss:  0.07302157580852509
0  loss:  0.06791627407073975
0  loss:  0.009787380695343018
0  loss:  0.22119708359241486
0  loss:  0.05527471750974655
0  loss:  0.052848681807518005
0  loss:  0.05139145255088806
0  loss:  0.04096660390496254
0  loss:  0.10424404591321945
0  loss:  0.17587868869304657
0  loss:  0.033819619566202164
0  loss:  0.029429368674755096
0  loss:  0.058232277631759644
0  loss:  0.02898116409778595
0  loss:  0.05785463750362396
0  loss:  0.08328968286514282
0  loss:  0.09082967042922974
0  loss:  0.06769510358572006
0  loss:  0.07673274725675583
0  loss:  0.03465583547949791
0  loss:  0.010527773760259151
0  loss:  0.11608520895242691
0  loss:  0.04617447778582573
0  loss:  0.24652592837810516
0  loss:  0.0728939026594162
0  loss:  0.08631216734647751
0  loss:  0.008507683873176575
0  loss:  0.012843403033912182
0  loss:  0.0869084894657135
0  loss:  0.05780600383877754
0  loss:  0.06943529844284058
------ 全部罪名 ------
Acc:  0.829835324507588
mi_f1:  0.7214921576939382
ma_f1:  0.7313672177231143
mi_precision:  0.7857802400738689
mi_recall:  0.6669278996865203
------ 交通肇事 ------
Acc:  0.9257294429708223
mi_f1:  0.8955223880597015
ma_f1:  0.9080558781778294
mi_precision:  0.8982035928143712
mi_recall:  0.8928571428571429
------ 抢劫 ------
Acc:  0.7671840354767184
mi_f1:  0.6507936507936508
ma_f1:  0.655774063356686
mi_precision:  0.6542553191489362
mi_recall:  0.6473684210526316
------ 抢夺 ------
Acc:  0.8448275862068966
mi_f1:  0.6819672131147541
ma_f1:  0.6822238630498366
mi_precision:  0.7272727272727273
mi_recall:  0.6419753086419753
------ 过失致人死亡 ------
Acc:  0.8454106280193237
mi_f1:  0.7329192546583853
ma_f1:  0.7418831602274851
mi_precision:  0.7712418300653595
mi_recall:  0.6982248520710059
------ 贪污 ------
Acc:  0.7941176470588235
mi_f1:  0.6457142857142858
ma_f1:  0.6677591599030054
mi_precision:  0.738562091503268
mi_recall:  0.5736040609137056
------ 挪用公款 ------
Acc:  0.805168986083499
mi_f1:  0.6866485013623979
ma_f1:  0.6873827452667813
mi_precision:  0.8344370860927153
mi_recall:  0.5833333333333334
------ 挪用资金 ------
Acc:  0.8544973544973545
mi_f1:  0.7748344370860927
ma_f1:  0.7399378570233515
mi_precision:  0.9140625
mi_recall:  0.6724137931034483
0   tensor(1.2635, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.2253, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.2529, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.2718, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.3017, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.3541, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.3925, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.4944, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.4527, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.4536, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5125, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.4657, device='cuda:0', grad_fn=<CopyBackwards>)
0.7375706378637286
