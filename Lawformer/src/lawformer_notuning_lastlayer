Some weights of the model checkpoint at ../../../RESOURCE/lawformer were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerModel were not initialized from the model checkpoint at ../../../RESOURCE/lawformer and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
start loading
end loading
681
681
133
133
133
133
149
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  0.8724167346954346
0  loss:  0.2372877448797226
0  loss:  0.2158738374710083
0  loss:  0.054963208734989166
0  loss:  0.10495410859584808
0  loss:  0.0743613913655281
0  loss:  0.14052632451057434
0  loss:  0.15476320683956146
0  loss:  0.08312971144914627
0  loss:  0.14933283627033234
0  loss:  0.12140269577503204
0  loss:  0.19416792690753937
0  loss:  0.11819794774055481
0  loss:  0.1452936977148056
0  loss:  0.0980730727314949
0  loss:  0.0986989438533783
0  loss:  0.09786231070756912
0  loss:  0.036962658166885376
0  loss:  0.06487536430358887
0  loss:  0.2500670254230499
0  loss:  0.10535450279712677
0  loss:  0.10252802073955536
0  loss:  0.14368212223052979
0  loss:  0.11671541631221771
0  loss:  0.13783369958400726
0  loss:  0.09809304028749466
0  loss:  0.08240991830825806
0  loss:  0.16372643411159515
0  loss:  0.09210038185119629
0  loss:  0.13669289648532867
0  loss:  0.04039845988154411
0  loss:  0.10127513855695724
0  loss:  0.07299603521823883
0  loss:  0.03833482041954994
0  loss:  0.18252559006214142
0  loss:  0.04799160361289978
0  loss:  0.11394661664962769
0  loss:  0.14804822206497192
0  loss:  0.1338079571723938
0  loss:  0.04120258241891861
0  loss:  0.05044786259531975
0  loss:  0.12309040129184723
0  loss:  0.14002393186092377
0  loss:  0.09604762494564056
0  loss:  0.1267842948436737
0  loss:  0.17128793895244598
0  loss:  0.10053125768899918
0  loss:  0.10893543064594269
0  loss:  0.15541954338550568
0  loss:  0.1399284452199936
0  loss:  0.04160816967487335
0  loss:  0.17543621361255646
0  loss:  0.07364638149738312
0  loss:  0.10306163132190704
0  loss:  0.2569030523300171
0  loss:  0.1490408480167389
0  loss:  0.07855310291051865
0  loss:  0.08269600570201874
0  loss:  0.06187110021710396
0  loss:  0.16328531503677368
0  loss:  0.09410080313682556
0  loss:  0.027059122920036316
0  loss:  0.05908036604523659
0  loss:  0.1056792140007019
0  loss:  0.026503097265958786
0  loss:  0.13341709971427917
0  loss:  0.1498793512582779
0  loss:  0.10024569183588028
0  loss:  0.04532942920923233
0  loss:  0.1904897540807724
0  loss:  0.14173941314220428
0  loss:  0.2432834804058075
0  loss:  0.11335641890764236
0  loss:  0.20865565538406372
0  loss:  0.2165168672800064
0  loss:  0.14383073151111603
0  loss:  0.16710542142391205
0  loss:  0.10717320442199707
0  loss:  0.20102457702159882
0  loss:  0.17714211344718933
0  loss:  0.12201441824436188
0  loss:  0.1600329726934433
0  loss:  0.03659693896770477
0  loss:  0.23849423229694366
0  loss:  0.11768561601638794
0  loss:  0.07617013901472092
0  loss:  0.11795969307422638
0  loss:  0.12496282905340195
0  loss:  0.1290527880191803
0  loss:  0.2227010726928711
0  loss:  0.24809031188488007
0  loss:  0.16094879806041718
0  loss:  0.07733780145645142
0  loss:  0.12487911432981491
0  loss:  0.19160722196102142
0  loss:  0.22073578834533691
0  loss:  0.19240842759609222
0  loss:  0.12919224798679352
0  loss:  0.12730728089809418
0  loss:  0.19308598339557648
------ 全部罪名 ------
Acc:  0.8022429906542056
mi_f1:  0.6482412060301508
ma_f1:  0.6485834712345454
mi_precision:  0.7465277777777778
mi_recall:  0.5728241563055062
------ 交通肇事 ------
Acc:  0.906801007556675
mi_f1:  0.8414239482200648
ma_f1:  0.8532634298361124
mi_precision:  0.8227848101265823
mi_recall:  0.8609271523178808
------ 抢劫 ------
Acc:  0.7258064516129032
mi_f1:  0.5889967637540453
ma_f1:  0.5535684837379753
mi_precision:  0.7109375
mi_recall:  0.5027624309392266
------ 抢夺 ------
Acc:  0.7849740932642487
mi_f1:  0.6127946127946129
ma_f1:  0.6234196911859147
mi_precision:  0.7109375
mi_recall:  0.5384615384615384
------ 过失致人死亡 ------
Acc:  0.8351063829787234
mi_f1:  0.6844106463878327
ma_f1:  0.6894909688013138
mi_precision:  0.7086614173228346
mi_recall:  0.6617647058823529
------ 贪污 ------
Acc:  0.7805429864253394
mi_f1:  0.5193798449612403
ma_f1:  0.5273869084844695
mi_precision:  0.6203703703703703
mi_recall:  0.44666666666666666
------ 挪用公款 ------
Acc:  0.7592067988668555
mi_f1:  0.5692883895131086
ma_f1:  0.5339943539993666
mi_precision:  0.7524752475247525
mi_recall:  0.4578313253012048
------ 挪用资金 ------
Acc:  0.8194842406876791
mi_f1:  0.6968641114982579
ma_f1:  0.6834087924241378
mi_precision:  0.8771929824561403
mi_recall:  0.5780346820809249
0   tensor(5.8653, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0928990840911865
0  loss:  0.34903520345687866
0  loss:  0.044415030628442764
0  loss:  0.20088523626327515
0  loss:  0.1638585776090622
0  loss:  0.14914019405841827
0  loss:  0.048445142805576324
0  loss:  0.1465938836336136
0  loss:  0.1069016382098198
0  loss:  0.13300813734531403
0  loss:  0.055191732943058014
0  loss:  0.10050804913043976
0  loss:  0.056524161249399185
0  loss:  0.0963163822889328
0  loss:  0.1356353610754013
0  loss:  0.10588157176971436
0  loss:  0.20692555606365204
0  loss:  0.24783985316753387
0  loss:  0.2093609869480133
0  loss:  0.26124370098114014
0  loss:  0.15051868557929993
0  loss:  0.17831167578697205
0  loss:  0.19783541560173035
0  loss:  0.36008113622665405
0  loss:  0.12902937829494476
0  loss:  0.07503388822078705
0  loss:  0.027231112122535706
0  loss:  0.17928795516490936
0  loss:  0.20949043333530426
0  loss:  0.04556197300553322
0  loss:  0.10213416814804077
0  loss:  0.10743249207735062
0  loss:  0.12106887251138687
0  loss:  0.0903133973479271
0  loss:  0.0667744129896164
0  loss:  0.11718400567770004
0  loss:  0.16077686846256256
0  loss:  0.11093655973672867
0  loss:  0.18382422626018524
0  loss:  0.09287294000387192
0  loss:  0.19229640066623688
0  loss:  0.17539942264556885
0  loss:  0.08335883170366287
0  loss:  0.04359889030456543
0  loss:  0.16694946587085724
0  loss:  0.19362229108810425
0  loss:  0.12338707596063614
0  loss:  0.051678143441677094
0  loss:  0.15676243603229523
0  loss:  0.14720602333545685
0  loss:  0.06096673011779785
0  loss:  0.09489595144987106
0  loss:  0.14113114774227142
0  loss:  0.04006559029221535
0  loss:  0.04240532964468002
0  loss:  0.10524553805589676
0  loss:  0.052858516573905945
0  loss:  0.06805671751499176
0  loss:  0.03657491132616997
0  loss:  0.21568408608436584
0  loss:  0.04593653604388237
0  loss:  0.16643960773944855
0  loss:  0.1220332458615303
0  loss:  0.08344229310750961
0  loss:  0.0863463506102562
0  loss:  0.06075689196586609
0  loss:  0.049181919544935226
0  loss:  0.12473369389772415
0  loss:  0.10557420551776886
0  loss:  0.09418017417192459
0  loss:  0.13372573256492615
0  loss:  0.23946185410022736
0  loss:  0.09266970306634903
0  loss:  0.06276992708444595
0  loss:  0.18424449861049652
0  loss:  0.07051631063222885
0  loss:  0.1068524420261383
0  loss:  0.22464430332183838
0  loss:  0.04850583150982857
0  loss:  0.07435812801122665
0  loss:  0.14522312581539154
0  loss:  0.18495313823223114
0  loss:  0.06866816431283951
0  loss:  0.1678057610988617
0  loss:  0.09301481395959854
0  loss:  0.11547138541936874
0  loss:  0.14557960629463196
0  loss:  0.07124345004558563
0  loss:  0.09306200593709946
0  loss:  0.07030946016311646
0  loss:  0.07348203659057617
0  loss:  0.1333034783601761
0  loss:  0.08908096700906754
0  loss:  0.17922906577587128
0  loss:  0.10893633216619492
0  loss:  0.12046145647764206
0  loss:  0.10060563683509827
0  loss:  0.10459334403276443
0  loss:  0.17750118672847748
0  loss:  0.2177334874868393
------ 全部罪名 ------
Acc:  0.8349846258968227
mi_f1:  0.6841541755888652
ma_f1:  0.6847073956876726
mi_precision:  0.7625298329355609
mi_recall:  0.6203883495145631
------ 交通肇事 ------
Acc:  0.9357142857142857
mi_f1:  0.8708487084870848
ma_f1:  0.8624464219664498
mi_precision:  0.8872180451127819
mi_recall:  0.855072463768116
------ 抢劫 ------
Acc:  0.7840172786177105
mi_f1:  0.652694610778443
ma_f1:  0.6512893311279186
mi_precision:  0.7171052631578947
mi_recall:  0.5989010989010989
------ 抢夺 ------
Acc:  0.8414634146341463
mi_f1:  0.6610169491525423
ma_f1:  0.6710291934685799
mi_precision:  0.7647058823529411
mi_recall:  0.582089552238806
------ 过失致人死亡 ------
Acc:  0.8461538461538461
mi_f1:  0.75177304964539
ma_f1:  0.7199347254494314
mi_precision:  0.7737226277372263
mi_recall:  0.7310344827586207
------ 贪污 ------
Acc:  0.8146551724137931
mi_f1:  0.546218487394958
ma_f1:  0.5536020307828147
mi_precision:  0.7471264367816092
mi_recall:  0.4304635761589404
------ 挪用公款 ------
Acc:  0.7833827893175074
mi_f1:  0.6180257510729614
ma_f1:  0.5889973339171803
mi_precision:  0.72
mi_recall:  0.5413533834586466
------ 挪用资金 ------
Acc:  0.8377192982456141
mi_f1:  0.6642335766423357
ma_f1:  0.6627066196631414
mi_precision:  0.7165354330708661
mi_recall:  0.6190476190476191
0   tensor(5.8109, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0139026641845703
0  loss:  0.2688680589199066
0  loss:  0.09334395080804825
0  loss:  0.06945737451314926
0  loss:  0.09363391995429993
0  loss:  0.07309722155332565
0  loss:  0.10475851595401764
0  loss:  0.11779750883579254
0  loss:  0.17055794596672058
0  loss:  0.09396860748529434
0  loss:  0.18606004118919373
0  loss:  0.09568294882774353
0  loss:  0.17912845313549042
0  loss:  0.08999654650688171
0  loss:  0.1316332370042801
0  loss:  0.049473367631435394
0  loss:  0.052194993942976
0  loss:  0.12424343079328537
0  loss:  0.13890746235847473
0  loss:  0.07832703739404678
0  loss:  0.058728206902742386
0  loss:  0.15845435857772827
0  loss:  0.09086966514587402
0  loss:  0.11952603608369827
0  loss:  0.159099280834198
0  loss:  0.15249493718147278
0  loss:  0.11790959537029266
0  loss:  0.05246252939105034
0  loss:  0.14633187651634216
0  loss:  0.03362616151571274
0  loss:  0.06541275233030319
0  loss:  0.06066641956567764
0  loss:  0.07056548446416855
0  loss:  0.2021883726119995
0  loss:  0.07789761573076248
0  loss:  0.24186550080776215
0  loss:  0.22498765587806702
0  loss:  0.15745988488197327
0  loss:  0.052617140114307404
0  loss:  0.3155308663845062
0  loss:  0.22235730290412903
0  loss:  0.17455780506134033
0  loss:  0.035458628088235855
0  loss:  0.11353907734155655
0  loss:  0.09090935438871384
0  loss:  0.09713003784418106
0  loss:  0.10917559266090393
0  loss:  0.09564346820116043
0  loss:  0.07461415976285934
0  loss:  0.05611812695860863
0  loss:  0.09512610733509064
0  loss:  0.19396710395812988
0  loss:  0.30166324973106384
0  loss:  0.06585106253623962
0  loss:  0.14187762141227722
0  loss:  0.04650218039751053
0  loss:  0.13619673252105713
0  loss:  0.1785290688276291
0  loss:  0.09391693025827408
0  loss:  0.0987401232123375
0  loss:  0.149614155292511
0  loss:  0.11329349130392075
0  loss:  0.11484590172767639
0  loss:  0.07932807505130768
0  loss:  0.12221632897853851
0  loss:  0.13393954932689667
0  loss:  0.08563173562288284
0  loss:  0.18491093814373016
0  loss:  0.11847305297851562
0  loss:  0.15427590906620026
0  loss:  0.254688024520874
0  loss:  0.12117227166891098
0  loss:  0.14873376488685608
0  loss:  0.08701854944229126
0  loss:  0.15789103507995605
0  loss:  0.12518596649169922
0  loss:  0.24105697870254517
0  loss:  0.08599507808685303
0  loss:  0.19134750962257385
0  loss:  0.1517009735107422
0  loss:  0.09579284489154816
0  loss:  0.12707141041755676
0  loss:  0.0987248495221138
0  loss:  0.08089478313922882
0  loss:  0.12914003431797028
0  loss:  0.10015084594488144
0  loss:  0.06962496787309647
0  loss:  0.10517186671495438
0  loss:  0.18323251605033875
0  loss:  0.07976683974266052
0  loss:  0.05155656859278679
0  loss:  0.0736604779958725
0  loss:  0.19405986368656158
0  loss:  0.13797315955162048
0  loss:  0.17279766499996185
0  loss:  0.13726302981376648
0  loss:  0.1563432663679123
0  loss:  0.032051827758550644
0  loss:  0.11232715845108032
0  loss:  0.12980110943317413
------ 全部罪名 ------
Acc:  0.8274660160334611
mi_f1:  0.6995967741935484
ma_f1:  0.7019261135187278
mi_precision:  0.7789001122334456
mi_recall:  0.6349496797804208
------ 交通肇事 ------
Acc:  0.9094736842105263
mi_f1:  0.8138801261829652
ma_f1:  0.8314066860858851
mi_precision:  0.8657718120805369
mi_recall:  0.7678571428571429
------ 抢劫 ------
Acc:  0.7766497461928934
mi_f1:  0.68
ma_f1:  0.6573294629898404
mi_precision:  0.7727272727272727
mi_recall:  0.6071428571428571
------ 抢夺 ------
Acc:  0.8225
mi_f1:  0.6925795053003534
ma_f1:  0.7112077889216046
mi_precision:  0.6901408450704225
mi_recall:  0.6950354609929078
------ 过失致人死亡 ------
Acc:  0.8201058201058201
mi_f1:  0.7054794520547946
ma_f1:  0.701133754305396
mi_precision:  0.8373983739837398
mi_recall:  0.6094674556213018
------ 贪污 ------
Acc:  0.7869249394673123
mi_f1:  0.6367041198501873
ma_f1:  0.6249962625345249
mi_precision:  0.7024793388429752
mi_recall:  0.5821917808219178
------ 挪用公款 ------
Acc:  0.8291457286432161
mi_f1:  0.7153284671532847
ma_f1:  0.6812920955383027
mi_precision:  0.8235294117647058
mi_recall:  0.632258064516129
------ 挪用资金 ------
Acc:  0.8321167883211679
mi_f1:  0.6294820717131474
ma_f1:  0.6237267634326458
mi_precision:  0.7523809523809524
mi_recall:  0.541095890410959
0   tensor(5.7388, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.055543303489685
0  loss:  0.35950660705566406
0  loss:  0.19357778131961823
0  loss:  0.15427166223526
0  loss:  0.03141387179493904
0  loss:  0.10317537188529968
0  loss:  0.07213078439235687
0  loss:  0.10204891860485077
0  loss:  0.15421558916568756
0  loss:  0.09820111095905304
0  loss:  0.1863027811050415
0  loss:  0.048287276178598404
0  loss:  0.09772942960262299
0  loss:  0.1109398901462555
0  loss:  0.15166881680488586
0  loss:  0.35594409704208374
0  loss:  0.1454663872718811
0  loss:  0.017500905320048332
0  loss:  0.11266051977872849
0  loss:  0.02190372161567211
0  loss:  0.11276206374168396
0  loss:  0.1305307000875473
0  loss:  0.08884496241807938
0  loss:  0.06797753274440765
0  loss:  0.10722872614860535
0  loss:  0.17673169076442719
0  loss:  0.15168620645999908
0  loss:  0.15694312751293182
0  loss:  0.15534372627735138
0  loss:  0.14847064018249512
0  loss:  0.07671360671520233
0  loss:  0.03312961384654045
0  loss:  0.15655235946178436
0  loss:  0.1040545105934143
0  loss:  0.1711101531982422
0  loss:  0.04490957781672478
0  loss:  0.2441955953836441
0  loss:  0.09665422141551971
0  loss:  0.10588545352220535
0  loss:  0.10404830425977707
0  loss:  0.15561534464359283
0  loss:  0.1384541541337967
0  loss:  0.19250327348709106
0  loss:  0.08396143466234207
0  loss:  0.16433517634868622
0  loss:  0.08927297592163086
0  loss:  0.17632915079593658
0  loss:  0.1777622550725937
0  loss:  0.0683705285191536
0  loss:  0.11206953227519989
0  loss:  0.09963224083185196
0  loss:  0.16351614892482758
0  loss:  0.05596478655934334
0  loss:  0.15491527318954468
0  loss:  0.10658691078424454
0  loss:  0.16393740475177765
0  loss:  0.11800850927829742
0  loss:  0.1189122274518013
0  loss:  0.1830909699201584
0  loss:  0.08746285736560822
0  loss:  0.07313275337219238
0  loss:  0.06338717043399811
0  loss:  0.1104341447353363
0  loss:  0.2852473855018616
0  loss:  0.09864538908004761
0  loss:  0.14677190780639648
0  loss:  0.09563390910625458
0  loss:  0.09880714118480682
0  loss:  0.18307726085186005
0  loss:  0.06843136250972748
0  loss:  0.038877177983522415
0  loss:  0.24789752066135406
0  loss:  0.17071287333965302
0  loss:  0.046708885580301285
0  loss:  0.044138628989458084
0  loss:  0.08953007310628891
0  loss:  0.23015543818473816
0  loss:  0.16883911192417145
0  loss:  0.16916488111019135
0  loss:  0.11074046045541763
0  loss:  0.07591798901557922
0  loss:  0.069488026201725
0  loss:  0.027620505541563034
0  loss:  0.0985039621591568
0  loss:  0.14923052489757538
0  loss:  0.33285215497016907
0  loss:  0.12654565274715424
0  loss:  0.1153167337179184
0  loss:  0.337782084941864
0  loss:  0.07757796347141266
0  loss:  0.24814973771572113
0  loss:  0.2661668658256531
0  loss:  0.10364995896816254
0  loss:  0.09699535369873047
0  loss:  0.10719382762908936
0  loss:  0.11881348490715027
0  loss:  0.13774354755878448
0  loss:  0.1781669408082962
0  loss:  0.10075071454048157
0  loss:  0.10479781776666641
------ 全部罪名 ------
Acc:  0.8280346820809249
mi_f1:  0.6876640419947506
ma_f1:  0.68291343438418
mi_precision:  0.8056580565805658
mi_recall:  0.5998168498168498
------ 交通肇事 ------
Acc:  0.9123711340206185
mi_f1:  0.8091603053435114
ma_f1:  0.8126625269667748
mi_precision:  0.8617886178861789
mi_recall:  0.762589928057554
------ 抢劫 ------
Acc:  0.7886363636363637
mi_f1:  0.5878136200716846
ma_f1:  0.5984189452610505
mi_precision:  0.7321428571428571
mi_recall:  0.49101796407185627
------ 抢夺 ------
Acc:  0.8287153652392947
mi_f1:  0.7122302158273381
ma_f1:  0.7210034625064483
mi_precision:  0.7857142857142857
mi_recall:  0.6513157894736842
------ 过失致人死亡 ------
Acc:  0.8270142180094787
mi_f1:  0.6594202898550725
ma_f1:  0.6459263854425145
mi_precision:  0.7844827586206896
mi_recall:  0.56875
------ 贪污 ------
Acc:  0.8011049723756906
mi_f1:  0.6738351254480287
ma_f1:  0.648628805611125
mi_precision:  0.831858407079646
mi_recall:  0.5662650602409639
------ 挪用公款 ------
Acc:  0.7984084880636605
mi_f1:  0.6486486486486486
ma_f1:  0.5991970926301555
mi_precision:  0.8235294117647058
mi_recall:  0.535031847133758
------ 挪用资金 ------
Acc:  0.8429319371727748
mi_f1:  0.7279411764705882
ma_f1:  0.7116764523393695
mi_precision:  0.8181818181818182
mi_recall:  0.6556291390728477
0   tensor(5.7641, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 768])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0713069438934326
0  loss:  0.23912262916564941
0  loss:  0.05386406183242798
0  loss:  0.08842694014310837
0  loss:  0.08262539654970169
0  loss:  0.0739155039191246
0  loss:  0.1390620470046997
0  loss:  0.13323915004730225
0  loss:  0.08814241737127304
0  loss:  0.07456091791391373
0  loss:  0.12079557776451111
0  loss:  0.07510469853878021
0  loss:  0.27407294511795044
0  loss:  0.20245976746082306
0  loss:  0.15609118342399597
0  loss:  0.2045096904039383
0  loss:  0.08611081540584564
0  loss:  0.07264617085456848
0  loss:  0.12298674881458282
0  loss:  0.2566089928150177
0  loss:  0.11013221740722656
0  loss:  0.07631470263004303
0  loss:  0.2464980185031891
0  loss:  0.17270039021968842
0  loss:  0.1935923844575882
0  loss:  0.0690024122595787
0  loss:  0.09122549742460251
0  loss:  0.1201133131980896
0  loss:  0.08117082715034485
0  loss:  0.09306760132312775
0  loss:  0.28267955780029297
0  loss:  0.13271236419677734
0  loss:  0.23974397778511047
0  loss:  0.02101357840001583
0  loss:  0.09393684566020966
0  loss:  0.20570708811283112
0  loss:  0.13801628351211548
0  loss:  0.11746422946453094
0  loss:  0.19935116171836853
0  loss:  0.23286186158657074
0  loss:  0.06510094553232193
0  loss:  0.06470926105976105
0  loss:  0.17454665899276733
0  loss:  0.05234982818365097
0  loss:  0.24513497948646545
0  loss:  0.1882685273885727
0  loss:  0.08578089624643326
0  loss:  0.10931340605020523
0  loss:  0.07347044348716736
0  loss:  0.16375470161437988
0  loss:  0.05570845305919647
0  loss:  0.04187243804335594
0  loss:  0.07491770386695862
0  loss:  0.0924009159207344
0  loss:  0.12189527601003647
0  loss:  0.10435761511325836
0  loss:  0.07691464573144913
0  loss:  0.21424943208694458
0  loss:  0.07144424319267273
0  loss:  0.14049631357192993
0  loss:  0.17193521559238434
0  loss:  0.06525576114654541
0  loss:  0.10582302510738373
0  loss:  0.08221189677715302
0  loss:  0.1612321138381958
0  loss:  0.13248847424983978
0  loss:  0.14088225364685059
0  loss:  0.14216534793376923
0  loss:  0.25291720032691956
0  loss:  0.14355383813381195
0  loss:  0.18754726648330688
0  loss:  0.1059579849243164
0  loss:  0.10390336811542511
0  loss:  0.10684217512607574
0  loss:  0.27451494336128235
0  loss:  0.085417240858078
0  loss:  0.09635937213897705
0  loss:  0.06409628689289093
0  loss:  0.10513252764940262
0  loss:  0.061454400420188904
0  loss:  0.035275187343358994
0  loss:  0.09843946993350983
0  loss:  0.08558543771505356
0  loss:  0.11229357123374939
0  loss:  0.12126567959785461
0  loss:  0.11699078977108002
0  loss:  0.14584684371948242
0  loss:  0.05178685486316681
0  loss:  0.06000981852412224
0  loss:  0.04695272818207741
0  loss:  0.09963265806436539
0  loss:  0.1673537641763687
0  loss:  0.13325783610343933
0  loss:  0.19369947910308838
0  loss:  0.0846463292837143
0  loss:  0.07739286124706268
0  loss:  0.10992659628391266
0  loss:  0.12023928016424179
0  loss:  0.21921056509017944
0  loss:  0.06799862533807755
------ 全部罪名 ------
Acc:  0.7939941879237972
mi_f1:  0.6493731085170774
ma_f1:  0.6591381660779444
mi_precision:  0.7242044358727098
mi_recall:  0.5885579937304075
------ 交通肇事 ------
Acc:  0.8806366047745358
mi_f1:  0.8198757763975155
ma_f1:  0.8236028623880708
mi_precision:  0.8571428571428571
mi_recall:  0.7857142857142857
------ 抢劫 ------
Acc:  0.7560975609756098
mi_f1:  0.6
ma_f1:  0.6075404489764491
mi_precision:  0.65625
mi_recall:  0.5526315789473685
------ 抢夺 ------
Acc:  0.8038793103448276
mi_f1:  0.5987261146496815
ma_f1:  0.6063325082508251
mi_precision:  0.618421052631579
mi_recall:  0.5802469135802469
------ 过失致人死亡 ------
Acc:  0.8260869565217391
mi_f1:  0.7239263803680982
ma_f1:  0.7303736246472194
mi_precision:  0.7515923566878981
mi_recall:  0.6982248520710059
------ 贪污 ------
Acc:  0.7411764705882353
mi_f1:  0.5061728395061729
ma_f1:  0.5424655735791808
mi_precision:  0.6456692913385826
mi_recall:  0.41624365482233505
------ 挪用公款 ------
Acc:  0.7554671968190855
mi_f1:  0.5828877005347594
ma_f1:  0.5749978001865442
mi_precision:  0.689873417721519
mi_recall:  0.5046296296296297
------ 挪用资金 ------
Acc:  0.828042328042328
mi_f1:  0.7326732673267328
ma_f1:  0.7040620229564182
mi_precision:  0.8604651162790697
mi_recall:  0.6379310344827587
0   tensor(4.9026, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)
0.6738058612648785
