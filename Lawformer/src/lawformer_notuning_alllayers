Some weights of the model checkpoint at ../../../RESOURCE/lawformer were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerModel were not initialized from the model checkpoint at ../../../RESOURCE/lawformer and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
start loading
end loading
681
681
133
133
133
133
149
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0121411085128784
0  loss:  0.347552090883255
0  loss:  0.2137160301208496
0  loss:  0.059801001101732254
0  loss:  0.20437675714492798
0  loss:  0.13342782855033875
0  loss:  0.03967250883579254
0  loss:  0.0615299828350544
0  loss:  0.11598360538482666
0  loss:  0.03240197151899338
0  loss:  0.17474432289600372
0  loss:  0.19657762348651886
0  loss:  0.03221755474805832
0  loss:  0.0158629659563303
0  loss:  0.1069197729229927
0  loss:  0.12460001558065414
0  loss:  0.19664889574050903
0  loss:  0.0990804135799408
0  loss:  0.12372870743274689
0  loss:  0.033398743718862534
0  loss:  0.09508605301380157
0  loss:  0.08560951799154282
0  loss:  0.14297454059123993
0  loss:  0.0515209399163723
0  loss:  0.08554932475090027
0  loss:  0.04557102918624878
0  loss:  0.1009855568408966
0  loss:  0.0330377034842968
0  loss:  0.05907018110156059
0  loss:  0.13179515302181244
0  loss:  0.15340903401374817
0  loss:  0.07533593475818634
0  loss:  0.07967453449964523
0  loss:  0.05406969040632248
0  loss:  0.09288696944713593
0  loss:  0.005346125457435846
0  loss:  0.03369466960430145
0  loss:  0.11766529828310013
0  loss:  0.029605930671095848
0  loss:  0.010012245737016201
0  loss:  0.07602859288454056
0  loss:  0.06486585736274719
0  loss:  0.10297460854053497
0  loss:  0.07788066565990448
0  loss:  0.05566851422190666
0  loss:  0.03921479359269142
0  loss:  0.03992198035120964
0  loss:  0.06437834352254868
0  loss:  0.09610119462013245
0  loss:  0.08717633038759232
0  loss:  0.06396752595901489
0  loss:  0.18071040511131287
0  loss:  0.1544511616230011
0  loss:  0.1370846927165985
0  loss:  0.06783289462327957
0  loss:  0.0884217619895935
0  loss:  0.08201014250516891
0  loss:  0.13806624710559845
0  loss:  0.027214013040065765
0  loss:  0.03445617854595184
0  loss:  0.07683627307415009
0  loss:  0.04491420462727547
0  loss:  0.04895554482936859
0  loss:  0.06625382602214813
0  loss:  0.07597403228282928
0  loss:  0.06747998297214508
0  loss:  0.15209005773067474
0  loss:  0.09511303156614304
0  loss:  0.004926362074911594
0  loss:  0.02221805416047573
0  loss:  0.04601002857089043
0  loss:  0.04867295175790787
0  loss:  0.08286505937576294
0  loss:  0.09109720587730408
0  loss:  0.09544308483600616
0  loss:  0.01900997944176197
0  loss:  0.05101536214351654
0  loss:  0.17879945039749146
0  loss:  0.11034932732582092
0  loss:  0.060789454728364944
0  loss:  0.1374332308769226
0  loss:  0.0760469064116478
0  loss:  0.04662039130926132
0  loss:  0.051002997905015945
0  loss:  0.23199693858623505
0  loss:  0.18985775113105774
0  loss:  0.14605192840099335
0  loss:  0.03721039742231369
0  loss:  0.06375974416732788
0  loss:  0.10810676217079163
0  loss:  0.039172232151031494
0  loss:  0.05345512926578522
0  loss:  0.13779260218143463
0  loss:  0.020124254748225212
0  loss:  0.031905848532915115
0  loss:  0.048456717282533646
0  loss:  0.06395398080348969
0  loss:  0.13764818012714386
0  loss:  0.08526237308979034
0  loss:  0.14576716721057892
------ 全部罪名 ------
Acc:  0.8291588785046728
mi_f1:  0.7215909090909091
ma_f1:  0.7296076354425602
mi_precision:  0.7728194726166329
mi_recall:  0.6767317939609236
------ 交通肇事 ------
Acc:  0.9370277078085643
mi_f1:  0.8896103896103896
ma_f1:  0.9030806125623871
mi_precision:  0.8726114649681529
mi_recall:  0.9072847682119205
------ 抢劫 ------
Acc:  0.7580645161290323
mi_f1:  0.6646153846153846
ma_f1:  0.6603520854037921
mi_precision:  0.75
mi_recall:  0.5966850828729282
------ 抢夺 ------
Acc:  0.7823834196891192
mi_f1:  0.6412698412698413
ma_f1:  0.6487968572020297
mi_precision:  0.6917808219178082
mi_recall:  0.5976331360946746
------ 过失致人死亡 ------
Acc:  0.8670212765957447
mi_f1:  0.7636363636363636
ma_f1:  0.7755743256385104
mi_precision:  0.7553956834532374
mi_recall:  0.7720588235294118
------ 贪污 ------
Acc:  0.8190045248868778
mi_f1:  0.6327272727272727
ma_f1:  0.6509917139688133
mi_precision:  0.696
mi_recall:  0.58
------ 挪用公款 ------
Acc:  0.7960339943342776
mi_f1:  0.6868686868686869
ma_f1:  0.6646828060083093
mi_precision:  0.7786259541984732
mi_recall:  0.6144578313253012
------ 挪用资金 ------
Acc:  0.839541547277937
mi_f1:  0.7697160883280758
ma_f1:  0.7773729341305408
mi_precision:  0.8472222222222222
mi_recall:  0.7052023121387283
0   tensor(1.5244, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.5331, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.5171, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.5942, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.7050, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.6607, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.6784, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.7235, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.7116, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.6402, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5868, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.6305, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  0.9950975179672241
0  loss:  0.359916627407074
0  loss:  0.07754626870155334
0  loss:  0.07321663945913315
0  loss:  0.08239217847585678
0  loss:  0.07967853546142578
0  loss:  0.024847213178873062
0  loss:  0.10339947789907455
0  loss:  0.06359423696994781
0  loss:  0.10943438857793808
0  loss:  0.16778334975242615
0  loss:  0.01695084385573864
0  loss:  0.028575025498867035
0  loss:  0.0794467031955719
0  loss:  0.04837300255894661
0  loss:  0.09257818013429642
0  loss:  0.10790015757083893
0  loss:  0.07463028281927109
0  loss:  0.10310449451208115
0  loss:  0.027386320754885674
0  loss:  0.01193215511739254
0  loss:  0.04510219022631645
0  loss:  0.115908682346344
0  loss:  0.112923763692379
0  loss:  0.07177229225635529
0  loss:  0.11589182913303375
0  loss:  0.06574002653360367
0  loss:  0.14846278727054596
0  loss:  0.039186667650938034
0  loss:  0.0686962679028511
0  loss:  0.018813734874129295
0  loss:  0.08251825720071793
0  loss:  0.020538851618766785
0  loss:  0.019031506031751633
0  loss:  0.09679334610700607
0  loss:  0.04482395946979523
0  loss:  0.03248511627316475
0  loss:  0.15556424856185913
0  loss:  0.12408540397882462
0  loss:  0.06794280558824539
0  loss:  0.05839464068412781
0  loss:  0.037752993404865265
0  loss:  0.1386813521385193
0  loss:  0.1266588568687439
0  loss:  0.04817795753479004
0  loss:  0.057021722197532654
0  loss:  0.058171048760414124
0  loss:  0.08459536731243134
0  loss:  0.11372767388820648
0  loss:  0.07113756984472275
0  loss:  0.02926652505993843
0  loss:  0.15273523330688477
0  loss:  0.20151469111442566
0  loss:  0.0614260658621788
0  loss:  0.10714882612228394
0  loss:  0.06861778348684311
0  loss:  0.05839462950825691
0  loss:  0.009813431650400162
0  loss:  0.02094198204576969
0  loss:  0.08827865868806839
0  loss:  0.04572535306215286
0  loss:  0.044843707233667374
0  loss:  0.06113390624523163
0  loss:  0.03830226883292198
0  loss:  0.1047259196639061
0  loss:  0.09582241624593735
0  loss:  0.047742679715156555
0  loss:  0.20183083415031433
0  loss:  0.06559964269399643
0  loss:  0.03077496774494648
0  loss:  0.046127911657094955
0  loss:  0.022698067128658295
0  loss:  0.09843607246875763
0  loss:  0.02996162511408329
0  loss:  0.06252849847078323
0  loss:  0.06046261265873909
0  loss:  0.06673072278499603
0  loss:  0.07576153427362442
0  loss:  0.06783100962638855
0  loss:  0.13331285119056702
0  loss:  0.04582115635275841
0  loss:  0.21683816611766815
0  loss:  0.04012443870306015
0  loss:  0.040899571031332016
0  loss:  0.1818765550851822
0  loss:  0.05630655959248543
0  loss:  0.04613431170582771
0  loss:  0.025538047775626183
0  loss:  0.03775127977132797
0  loss:  0.04565085098147392
0  loss:  0.03200230374932289
0  loss:  0.08664298802614212
0  loss:  0.08088470995426178
0  loss:  0.09374313056468964
0  loss:  0.10038953274488449
0  loss:  0.04119237884879112
0  loss:  0.14522190392017365
0  loss:  0.022220194339752197
0  loss:  0.130381241440773
0  loss:  0.10212475806474686
------ 全部罪名 ------
Acc:  0.8483088486504954
mi_f1:  0.7429120615088899
ma_f1:  0.7471729003274779
mi_precision:  0.7354900095147479
mi_recall:  0.7504854368932039
------ 交通肇事 ------
Acc:  0.9380952380952381
mi_f1:  0.8732394366197183
ma_f1:  0.8797774004904735
mi_precision:  0.8493150684931506
mi_recall:  0.8985507246376812
------ 抢劫 ------
Acc:  0.7883369330453563
mi_f1:  0.6759002770083102
ma_f1:  0.70366927592955
mi_precision:  0.6815642458100558
mi_recall:  0.6703296703296703
------ 抢夺 ------
Acc:  0.8317073170731707
mi_f1:  0.697841726618705
ma_f1:  0.7043266349940078
mi_precision:  0.6736111111111112
mi_recall:  0.7238805970149254
------ 过失致人死亡 ------
Acc:  0.8514588859416445
mi_f1:  0.782608695652174
ma_f1:  0.7820800969504398
mi_precision:  0.7597402597402597
mi_recall:  0.8068965517241379
------ 贪污 ------
Acc:  0.8599137931034483
mi_f1:  0.7272727272727273
ma_f1:  0.7056828288342955
mi_precision:  0.8064516129032258
mi_recall:  0.6622516556291391
------ 挪用公款 ------
Acc:  0.8189910979228486
mi_f1:  0.7164179104477612
ma_f1:  0.7012644605864944
mi_precision:  0.7111111111111111
mi_recall:  0.7218045112781954
------ 挪用资金 ------
Acc:  0.8486842105263158
mi_f1:  0.740506329113924
ma_f1:  0.7434675484860499
mi_precision:  0.6923076923076923
mi_recall:  0.7959183673469388
0   tensor(1.4462, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.4746, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.4793, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.5467, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.6464, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.6099, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.6308, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.6352, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.6080, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.5592, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5132, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.5666, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0218675136566162
0  loss:  0.3634835481643677
0  loss:  0.18283557891845703
0  loss:  0.08431433886289597
0  loss:  0.16268639266490936
0  loss:  0.045049648731946945
0  loss:  0.042563315480947495
0  loss:  0.07335391640663147
0  loss:  0.06166985258460045
0  loss:  0.07177939265966415
0  loss:  0.0660901740193367
0  loss:  0.13967737555503845
0  loss:  0.034315504133701324
0  loss:  0.01602328196167946
0  loss:  0.0712033063173294
0  loss:  0.030092235654592514
0  loss:  0.06963697820901871
0  loss:  0.06690693646669388
0  loss:  0.02346574328839779
0  loss:  0.06516257673501968
0  loss:  0.10605847835540771
0  loss:  0.061571333557367325
0  loss:  0.021724823862314224
0  loss:  0.03933502733707428
0  loss:  0.037133317440748215
0  loss:  0.015180403366684914
0  loss:  0.16037589311599731
0  loss:  0.08391764014959335
0  loss:  0.06542602926492691
0  loss:  0.02369844727218151
0  loss:  0.1653791069984436
0  loss:  0.06593416631221771
0  loss:  0.08413852006196976
0  loss:  0.16937777400016785
0  loss:  0.06620370596647263
0  loss:  0.01947532780468464
0  loss:  0.11040697991847992
0  loss:  0.04889998584985733
0  loss:  0.07275594025850296
0  loss:  0.07933283597230911
0  loss:  0.047651275992393494
0  loss:  0.07829643785953522
0  loss:  0.04563458636403084
0  loss:  0.009968062862753868
0  loss:  0.11288709193468094
0  loss:  0.043083690106868744
0  loss:  0.15705709159374237
0  loss:  0.08778742700815201
0  loss:  0.16749882698059082
0  loss:  0.13795457780361176
0  loss:  0.11597629636526108
0  loss:  0.05125948041677475
0  loss:  0.04882561042904854
0  loss:  0.08045510202646255
0  loss:  0.061470355838537216
0  loss:  0.13233794271945953
0  loss:  0.15102624893188477
0  loss:  0.05862095206975937
0  loss:  0.042147327214479446
0  loss:  0.10989492386579514
0  loss:  0.02079901099205017
0  loss:  0.02673041634261608
0  loss:  0.1078321784734726
0  loss:  0.08805175870656967
0  loss:  0.09678024798631668
0  loss:  0.09034658223390579
0  loss:  0.09706006199121475
0  loss:  0.05565215274691582
0  loss:  0.08084652572870255
0  loss:  0.17503851652145386
0  loss:  0.08671361207962036
0  loss:  0.038428667932748795
0  loss:  0.07309947907924652
0  loss:  0.07781190425157547
0  loss:  0.058509036898612976
0  loss:  0.13441069424152374
0  loss:  0.06629383563995361
0  loss:  0.12295512109994888
0  loss:  0.05045560374855995
0  loss:  0.04617343470454216
0  loss:  0.029897956177592278
0  loss:  0.1362510323524475
0  loss:  0.07522978633642197
0  loss:  0.075398288667202
0  loss:  0.027671577408909798
0  loss:  0.11358852684497833
0  loss:  0.04562288895249367
0  loss:  0.10197406262159348
0  loss:  0.0698084831237793
0  loss:  0.14732995629310608
0  loss:  0.06516370922327042
0  loss:  0.017452899366617203
0  loss:  0.0059813461266458035
0  loss:  0.06936844438314438
0  loss:  0.0965401828289032
0  loss:  0.04305877536535263
0  loss:  0.02427307702600956
0  loss:  0.07446242868900299
0  loss:  0.053532958030700684
0  loss:  0.13384981453418732
------ 全部罪名 ------
Acc:  0.8539560822586267
mi_f1:  0.7657920310981535
ma_f1:  0.7748976380199657
mi_precision:  0.816580310880829
mi_recall:  0.7209515096065874
------ 交通肇事 ------
Acc:  0.9221052631578948
mi_f1:  0.8606060606060606
ma_f1:  0.8767731589124049
mi_precision:  0.8765432098765432
mi_recall:  0.8452380952380952
------ 抢劫 ------
Acc:  0.8197969543147208
mi_f1:  0.7500000000000001
ma_f1:  0.7400621807683765
mi_precision:  0.8382352941176471
mi_recall:  0.6785714285714286
------ 抢夺 ------
Acc:  0.815
mi_f1:  0.6884057971014493
ma_f1:  0.7082850054125489
mi_precision:  0.7037037037037037
mi_recall:  0.6737588652482269
------ 过失致人死亡 ------
Acc:  0.8492063492063492
mi_f1:  0.7784810126582278
ma_f1:  0.7983774038461539
mi_precision:  0.8367346938775511
mi_recall:  0.727810650887574
------ 贪污 ------
Acc:  0.8256658595641646
mi_f1:  0.7303754266211604
ma_f1:  0.7421045799544905
mi_precision:  0.7278911564625851
mi_recall:  0.7328767123287672
------ 挪用公款 ------
Acc:  0.8668341708542714
mi_f1:  0.7841726618705037
ma_f1:  0.7853310490572232
mi_precision:  0.8861788617886179
mi_recall:  0.7032258064516129
------ 挪用资金 ------
Acc:  0.8661800486618005
mi_f1:  0.7509578544061303
ma_f1:  0.7500245098039215
mi_precision:  0.8521739130434782
mi_recall:  0.6712328767123288
0   tensor(1.4694, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.4949, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.4895, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.5935, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.6661, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.6302, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.6507, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.6999, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.6562, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.5884, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5526, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.6011, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.016709804534912
0  loss:  0.38012978434562683
0  loss:  0.20414116978645325
0  loss:  0.3106740713119507
0  loss:  0.08112897723913193
0  loss:  0.06409597396850586
0  loss:  0.10788389295339584
0  loss:  0.1471218317747116
0  loss:  0.11589696258306503
0  loss:  0.16977514326572418
0  loss:  0.024752827361226082
0  loss:  0.024395816028118134
0  loss:  0.10582029819488525
0  loss:  0.06669647991657257
0  loss:  0.036703694611787796
0  loss:  0.06980080902576447
0  loss:  0.07302781939506531
0  loss:  0.07435339689254761
0  loss:  0.06814068555831909
0  loss:  0.07671495527029037
0  loss:  0.05280648171901703
0  loss:  0.04328946769237518
0  loss:  0.06212328374385834
0  loss:  0.03008817508816719
0  loss:  0.0878077894449234
0  loss:  0.05596287548542023
0  loss:  0.07653345167636871
0  loss:  0.08706477284431458
0  loss:  0.0518338568508625
0  loss:  0.025062454864382744
0  loss:  0.015629753470420837
0  loss:  0.039771225303411484
0  loss:  0.18986637890338898
0  loss:  0.02955368161201477
0  loss:  0.040216539055109024
0  loss:  0.06576664745807648
0  loss:  0.0874708890914917
0  loss:  0.05157577618956566
0  loss:  0.0794496163725853
0  loss:  0.08611147850751877
0  loss:  0.030064603313803673
0  loss:  0.03723583742976189
0  loss:  0.14899411797523499
0  loss:  0.10744276642799377
0  loss:  0.06601545214653015
0  loss:  0.05899882689118385
0  loss:  0.030913904309272766
0  loss:  0.12071593850851059
0  loss:  0.15566687285900116
0  loss:  0.17061905562877655
0  loss:  0.08305622637271881
0  loss:  0.06073111668229103
0  loss:  0.04229334741830826
0  loss:  0.06433884799480438
0  loss:  0.016563933342695236
0  loss:  0.06139163300395012
0  loss:  0.07857533544301987
0  loss:  0.14773550629615784
0  loss:  0.09494158625602722
0  loss:  0.0898803099989891
0  loss:  0.12325181066989899
0  loss:  0.07581451535224915
0  loss:  0.07676434516906738
0  loss:  0.06346382200717926
0  loss:  0.04574371501803398
0  loss:  0.21061170101165771
0  loss:  0.05605524405837059
0  loss:  0.1350107342004776
0  loss:  0.0919065773487091
0  loss:  0.05940035730600357
0  loss:  0.06694170832633972
0  loss:  0.0890745222568512
0  loss:  0.0653340071439743
0  loss:  0.03768328204751015
0  loss:  0.020431099459528923
0  loss:  0.1540483981370926
0  loss:  0.06981153786182404
0  loss:  0.09723827242851257
0  loss:  0.012586855329573154
0  loss:  0.0679280012845993
0  loss:  0.11976258456707001
0  loss:  0.05760335177183151
0  loss:  0.031040485948324203
0  loss:  0.12743861973285675
0  loss:  0.1435542106628418
0  loss:  0.06613217294216156
0  loss:  0.04979701712727547
0  loss:  0.06273868680000305
0  loss:  0.08834724128246307
0  loss:  0.052121177315711975
0  loss:  0.1320926547050476
0  loss:  0.08039414137601852
0  loss:  0.12121445685625076
0  loss:  0.1450466811656952
0  loss:  0.02662610076367855
0  loss:  0.044575564563274384
0  loss:  0.2123890519142151
0  loss:  0.09158504754304886
0  loss:  0.10019394010305405
0  loss:  0.041634488850831985
------ 全部罪名 ------
Acc:  0.8583815028901735
mi_f1:  0.7542627883650952
ma_f1:  0.7621011100924128
mi_precision:  0.8337028824833703
mi_recall:  0.6886446886446886
------ 交通肇事 ------
Acc:  0.9329896907216495
mi_f1:  0.8686131386861314
ma_f1:  0.8785329421690443
mi_precision:  0.8814814814814815
mi_recall:  0.8561151079136691
------ 抢劫 ------
Acc:  0.8181818181818182
mi_f1:  0.6597222222222223
ma_f1:  0.687990093398063
mi_precision:  0.7851239669421488
mi_recall:  0.5688622754491018
------ 抢夺 ------
Acc:  0.8463476070528967
mi_f1:  0.7247386759581882
ma_f1:  0.7382895711771787
mi_precision:  0.7703703703703704
mi_recall:  0.6842105263157895
------ 过失致人死亡 ------
Acc:  0.8625592417061612
mi_f1:  0.7383512544802867
ma_f1:  0.7377295855788357
mi_precision:  0.865546218487395
mi_recall:  0.64375
------ 贪污 ------
Acc:  0.8259668508287292
mi_f1:  0.7491638795986623
ma_f1:  0.7573535076315663
mi_precision:  0.8421052631578947
mi_recall:  0.6746987951807228
------ 挪用公款 ------
Acc:  0.8381962864721485
mi_f1:  0.7404844290657439
ma_f1:  0.7174344394422199
mi_precision:  0.8106060606060606
mi_recall:  0.6815286624203821
------ 挪用资金 ------
Acc:  0.887434554973822
mi_f1:  0.8057553956834532
ma_f1:  0.8092132380031316
mi_precision:  0.8818897637795275
mi_recall:  0.7417218543046358
0   tensor(1.4792, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.5072, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.5019, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.5977, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.6733, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.6506, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.6867, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.7154, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.7003, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.6162, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5552, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.5926, device='cuda:0', grad_fn=<CopyBackwards>)
dataset loaded
linear.weight   torch.Size([768, 768])
linear.bias   torch.Size([768])
Classify_layer.weight   torch.Size([4, 9216])
Classify_layer.bias   torch.Size([4])
start training
0  loss:  1.0096462965011597
0  loss:  0.38974660634994507
0  loss:  0.1502632200717926
0  loss:  0.05790751799941063
0  loss:  0.092242531478405
0  loss:  0.08831485360860825
0  loss:  0.07185208052396774
0  loss:  0.08057019114494324
0  loss:  0.047869980335235596
0  loss:  0.04744669422507286
0  loss:  0.0847325399518013
0  loss:  0.014415738172829151
0  loss:  0.06728681921958923
0  loss:  0.1329774409532547
0  loss:  0.08159337937831879
0  loss:  0.03113148920238018
0  loss:  0.059532567858695984
0  loss:  0.03339846059679985
0  loss:  0.010972057469189167
0  loss:  0.08260595053434372
0  loss:  0.07756928354501724
0  loss:  0.09681779146194458
0  loss:  0.07395094633102417
0  loss:  0.016415411606431007
0  loss:  0.07733707875013351
0  loss:  0.11460380256175995
0  loss:  0.0844484195113182
0  loss:  0.04330279305577278
0  loss:  0.06621190160512924
0  loss:  0.05440805107355118
0  loss:  0.042670492082834244
0  loss:  0.05601195618510246
0  loss:  0.1286720633506775
0  loss:  0.09782786667346954
0  loss:  0.11300196498632431
0  loss:  0.1204104945063591
0  loss:  0.02742115408182144
0  loss:  0.07593104243278503
0  loss:  0.015844004228711128
0  loss:  0.0642920657992363
0  loss:  0.06381159275770187
0  loss:  0.055904459208250046
0  loss:  0.06911233067512512
0  loss:  0.045330870896577835
0  loss:  0.08930686861276627
0  loss:  0.08608193695545197
0  loss:  0.13886582851409912
0  loss:  0.09781081974506378
0  loss:  0.012079079635441303
0  loss:  0.05558203160762787
0  loss:  0.07085207849740982
0  loss:  0.1254924088716507
0  loss:  0.036408860236406326
0  loss:  0.07649747282266617
0  loss:  0.11734659969806671
0  loss:  0.07740019261837006
0  loss:  0.06208640709519386
0  loss:  0.035325102508068085
0  loss:  0.02628684788942337
0  loss:  0.08630768954753876
0  loss:  0.06888383626937866
0  loss:  0.15994758903980255
0  loss:  0.054357822984457016
0  loss:  0.07911131531000137
0  loss:  0.06248650327324867
0  loss:  0.018618091940879822
0  loss:  0.052541643381118774
0  loss:  0.1664266139268875
0  loss:  0.09256961941719055
0  loss:  0.015726260840892792
0  loss:  0.1761338859796524
0  loss:  0.03624270111322403
0  loss:  0.11610525101423264
0  loss:  0.0033323881216347218
0  loss:  0.1243176981806755
0  loss:  0.02957523614168167
0  loss:  0.09600616991519928
0  loss:  0.014299236238002777
0  loss:  0.13508515059947968
0  loss:  0.07395396381616592
0  loss:  0.05022349953651428
0  loss:  0.03380027413368225
0  loss:  0.056066036224365234
0  loss:  0.02503219060599804
0  loss:  0.02482915297150612
0  loss:  0.07489404082298279
0  loss:  0.06584389507770538
0  loss:  0.09067831933498383
0  loss:  0.04731011763215065
0  loss:  0.08080810308456421
0  loss:  0.0812624990940094
0  loss:  0.0476529635488987
0  loss:  0.09230345487594604
0  loss:  0.010356111451983452
0  loss:  0.1332794576883316
0  loss:  0.14901484549045563
0  loss:  0.014089517295360565
0  loss:  0.01927158422768116
0  loss:  0.09640878438949585
0  loss:  0.05799927935004234
------ 全部罪名 ------
Acc:  0.8346787213432354
mi_f1:  0.7288280581693756
ma_f1:  0.7305615434891255
mi_precision:  0.8022598870056498
mi_recall:  0.6677115987460815
------ 交通肇事 ------
Acc:  0.9230769230769231
mi_f1:  0.8895705521472392
ma_f1:  0.9001654954280541
mi_precision:  0.9177215189873418
mi_recall:  0.8630952380952381
------ 抢劫 ------
Acc:  0.7494456762749445
mi_f1:  0.6216216216216216
ma_f1:  0.618402337292892
mi_precision:  0.6388888888888888
mi_recall:  0.6052631578947368
------ 抢夺 ------
Acc:  0.8362068965517241
mi_f1:  0.6855345911949685
ma_f1:  0.6886910580458968
mi_precision:  0.6987179487179487
mi_recall:  0.6728395061728395
------ 过失致人死亡 ------
Acc:  0.8647342995169082
mi_f1:  0.7667731629392972
ma_f1:  0.7637520899583647
mi_precision:  0.8333333333333334
mi_recall:  0.7100591715976331
------ 贪污 ------
Acc:  0.7941176470588235
mi_f1:  0.6200607902735562
ma_f1:  0.630515318015318
mi_precision:  0.7727272727272727
mi_recall:  0.5177664974619289
------ 挪用公款 ------
Acc:  0.8250497017892644
mi_f1:  0.7049180327868853
ma_f1:  0.699311316082704
mi_precision:  0.86
mi_recall:  0.5972222222222222
------ 挪用资金 ------
Acc:  0.8809523809523809
mi_f1:  0.8354430379746836
ma_f1:  0.7900690256414002
mi_precision:  0.9295774647887324
mi_recall:  0.7586206896551724
0   tensor(1.4218, device='cuda:0', grad_fn=<CopyBackwards>)
1   tensor(1.4647, device='cuda:0', grad_fn=<CopyBackwards>)
2   tensor(1.4684, device='cuda:0', grad_fn=<CopyBackwards>)
3   tensor(1.5671, device='cuda:0', grad_fn=<CopyBackwards>)
4   tensor(1.6390, device='cuda:0', grad_fn=<CopyBackwards>)
5   tensor(1.5890, device='cuda:0', grad_fn=<CopyBackwards>)
6   tensor(1.6045, device='cuda:0', grad_fn=<CopyBackwards>)
7   tensor(1.6371, device='cuda:0', grad_fn=<CopyBackwards>)
8   tensor(1.6054, device='cuda:0', grad_fn=<CopyBackwards>)
9   tensor(1.5500, device='cuda:0', grad_fn=<CopyBackwards>)
10   tensor(1.5204, device='cuda:0', grad_fn=<CopyBackwards>)
11   tensor(1.5553, device='cuda:0', grad_fn=<CopyBackwards>)
0.7426771696464847
